#!/bin/bash -l
#SBATCH -A motorBile
#SBATCH -J test
#SBATCH -e error_%J.e
#SBATCH -o output_%J.o
#SBATCH -t 10:00:00
#
export SCRIPT_VERSION="Ofoam.x.96b"
export MPT_VERSION=mpt-2.27 # This is the runtime MPT version
export HMPT_VERSION=hmpt-2.27 # This is the runtime HMPT version
export UCX_RUNTIME_VERSION="ucx-1.13.1"
#export MPI_SECRET_AVOID_DOCOW=true
ulimit -c 0
ulimit -n 4096
ulimit -a
export OPENFOAM_D_DEFAULT=/lus/scratch/smehta/motorBike_s/OpenFOAM_HPE_Rhel8.6_Craype2.7.17_V2012_BASERELEASE_O3DEFAULT
#
if [ "$1" = "-v" ]; then
     echo SCRIPT_VERSION=$SCRIPT_VERSION
     echo Default Version: $OPENFOAM_D_DEFAULT
     if [ ! -d $OPENFOAM_D_DEFAULT ]; then
        echo "Warning: $OPENFOAM_D_DEFAULT does not exist!"
     fi
     exit
fi
echo "*** SCRIPT_VERSION=$SCRIPT_VERSION"
#
if [ "$1" = "-h" -o "$1" = "-help" ]; then
echo
echo 'Ofoam.x is a generic script designed to submit OpenFOAM jobs to a PBSpro batch schedular queue'
echo 'using the PBSpro qsub command.'
echo 
echo 'PBSpro qsub command line must specify in its -v option the following keywords and their values:'
echo
echo '   APPRC         : A user supplied full path to an rc file containing keywords script definitions'
echo '                   most important is OPENFOAM_D which is a pointer to the OpenFOAM installation to be'
echo '                   used by the job. This script has an internal default for OPENFOAM_D'
echo '   NPROC         : Number mpi tasks for the job'
echo '                   if NPROC is not set and the keyword EXE=snappyHexMesh is set '
echo '                   then snappyHexMesh will run serially '
echo '                   otherwise snappyHexMesh will run parallel using NPROC processes '
echo '   SCRATCH_DIR   : Full path to job working directory, default $PBS_O_WORKDIR'
echo ' PRECISION       : OpenFOAM Precision (SP or DP), default is DP'
echo '                 : Has no effect if using a module file because WM_PRECISION_OPTION is defined in the module file'
echo '   MPITYPE       : SGIMPI or HMPT or INTELMPI or HPCX_OPENMPI (HPCX), OPENMPI and CRAY-MPICH, default is SGIMPI, new is HMPT'
echo ' CRAY_MPICH_USE_SRUN: if set, CRAY-MPICH will use srun command line rather than mpiexec'
echo ' CRAY_MPICH_USE_MPIEXEC: if set, CRAY-MPICH will use mpiexec'
echo ' FOAM_MODULE_FILE    : module file name, must exist in $HOME/modulefiles directory'
echo ' CRAY_SLURM_TURBO_OFF: request to set TURBO off on Cray Raptor Cluster'
echo '                     : Note that the default on Cray Raptor is TURBO ON'
echo '                     : When is option is set, the job will run with TURBO off '
echo '                     : and at the end of the job TURBO will be set back to ON'
echo ' FIRST_NODE_NUM_CORES: Number of tasks to invoke on the first node.'
echo '                     : This is used in SLURM when the number of tasks is not a multipul of the number of cores per node'
echo '                     : Thus FIRST_NODE_NUM_CORES is the remainder of the division'
echo '   BY_NODE       : If set, the PBS_NODEFILE will be re-made by nodeIDtaskID rather than TaskIDnodeID'
echo ' FOAM_DOT_FILE_SETUP: if set the script will use internal code to source foamdotfile'
echo '                      Default is to source $FOAM_INST_DIR/setup.sh Precision MPITYPE'
echo '   CPFILES       : coma seperated list of files to be copied to the decompositioned domain directories'
echo '                 : Multi Copies only supported with HPCX OPENMPI'
echo '   COPIES        : An integer specifying the number of copies to be generated for a particular'
echo '                 : domain decomposition. The default action for a domain decomposition is to'
echo '                 : generate one copy under name $NPROC_d. When COPIES is set (for example COPIES=2)'
echo '                 : the script will make two identical copies of the domain decomposition directory'
echo '                 : under names $NPROC_d_1 and $NPROC_d_2. These sub-directories can be used later'
echo '                 : by the solver to run jobs with same NPROC number concurently'
echo ' CCX_TEST        : Used with Multi Copies Test (see COPIES)'
echo '                 : =1 test1 (default)'
echo '                 : =2 test2'
echo ''
echo '   The following three environment variables affect OPENMPI only if using mlx5 fabric:'
echo '   FABRIC        : Set to either MLX or OPA'
echo ''
echo '   MXM_KIND      : UD or RC or DC, default empty_string, currently not used by this script'
echo '   DECOMP        : 0 no decomp 1 decomp only 2 decomp+solver '
echo '   EXE           : Name of OpenFOAM executable to be used in the Solver, e.g simpleFoam, interfoam, etc'
echo '                   default EXE is simpleFoam'
echo '   FOAM_OPTIONS  : args to be passed to EXE'
echo '   MPTPIN        : if set MPT pinning will be done by default by MPT'
echo '                 : if not set then CORES need be set so that MPI_DSM_CPULIST is used'
echo '  Pinning Options selectors:'
echo ''
echo ' - Pinning Options with MPT:'
echo '   The default behaviour is to set MPI_DSM_DISTRIBUTE sgi mpi environment variable'
echo '   CORES         : Range of core ids for SGIMPI per node thread-to-core placement, '
echo '                   e.g 0-23 or 0-15, 0-19, 0-47, 1-9\,14-21 '
echo '   CORES1,CORES2 : CORES1 is range on 1st socket, CORES2 is range on 2nd socket'
echo '                 : CORES is ignorerd when CORES1 and CORES2 are set'
echo '   CORES[345678] : Used with CORES1 and CORES2 for future intel cpus with more than 2 sockets'
echo '   AMD7502       : Specifies AMD EPYC 7543 & 7502 & 7542 & 7532 & 7601 pinning archeticture, it works for all 32Core/Socket AMDs'
echo '   AMD7702       : Specifies AMD EPYC 7702 & 7742 pinning archeticture'
echo '                   When any of AMD7502 or AMD7702 are set, the user can then set:'
echo '   Use: ALT1, ALT1HT, ALT2, ALT2HT, ALT4, ALT4HT, ALT8, ALT8HT, ALT16 & ALT16HT, ALT96'
echo '   the integer x in the ALTx is the number of cores to alternate between pinnings'
echo '   there is also $ALT4MBcashPCore and $ALT8MBcashPCore inside the AMD7702/7742 set'
echo '   to emualate the the use of 4MB and 8MB cache per core pinning'
echo ''
echo ' - Pinning Options with MLX_OPENMPI'
echo '   As in Pinning Options for MPT above with the added CORES settings:'
echo '     CORES=socket : to pin based across sockets using --bind-to socket'
echo '     CORES=core   : to pin based successive core ids using --bind-to core'
echo ''
echo '  Pinning Options with IMPI: '
echo ''
echo '  The AMD7502 and AMD7702 along with the ALTx options above also apply to IMPI'
echo '  For Intel processors, because IMPI donot use wild cards or ranges for pinnings'
echo '  users can supply a PPN keyword as follows:'
echo '  PPN         :  When using PMPI or IMPI, mpi ranks may be pinned to cores based on PPN value:'
echo '              :  Valid values are 128, 88, 72, 64, 56, 48, 44, 40, 36, 32, 28, 24, 20, 16, 12, 8, 6 and 4'
echo '              : 128 : 0,1,2,3,...,126,127'
echo '              :  88 : 0,1,2,3,...,86,87'
echo '              :  72 : 0,1,2,3,...,70,71'
echo '              :  64 : 0,1,2,3,...,62,63'
echo '              :  56 : 0,1,2,3,...,54,55'
echo '              :  48 : 0,1,2,3,...,46,47'
echo '              :  44 : 0,1,2,3,...,42,43'
echo '              :  40 : 0,1,2,3,...,38,39'
echo '              :  36 : 0,1,2,3,...,34,35'
echo '              :  32 : 0,1,2,3,...,30,31'
echo '              :  28 : 0,1,2,3,...,26,27'
echo '              :  24 : 0,1,2,3,...,22,23'
echo '              :  20 : 0,1,2,3,...,18,19'
echo '              :  16 : 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15'
echo '              :  12 : 0,1,2,3,4,5,6,7,8,9,10,11'
echo '              :   8 : 0,1,2,3,4,5,6,7'
echo '              :   6 : 0,1,2,3,4,5'
echo '              :   4 : 0,1,2,3'
echo '              :   Default is 128'
echo '  USER_PPN    : A string of cores id list supplied by the user own pin selection'
echo ''
echo '   COLL_OPTION   : if set to YES,Y,ON,1 enables SHArP HCOLL for MPT or OMPI'
echo ' SERIAL          : If set, will run a solver EXE in serial mode, no prior decomposition '
echo ' DUAL_RAIL       : If set will set the SGIMPI env var MPI_IB_RAIL=2 to use dual rail on ICE systems'
echo '  USE_LDISK      : A path to run the input model from the local disk file system of every compute'
echo '                   node in the cluster. UV not supported.'
echo '  USE_DEVSHM     : if set will run the input model from the /dev/shm file system of every compute'
echo '                   node in the cluster. UV not supported.'
echo '  CP_DEVSHM_RES  : If set will copy back the simulation results from each compute node /dev/shm or from the path in USE_LDISK '
echo '                   back to the main input case directory. Setting this keyword is only effective '
echo '                   when USE_DEVSHM has been set '
echo '   IB_DEV        : 0 for ib0, or 1 for ib1 '
echo '                   which sets MPI_IB_DEVS SGI MPT env Var accordingly '
echo '   MACH          : Spec of Executing hardware plus other job related info so that the output'
echo '                   file reflects what the job was about'
echo '     OPENSS_PROF : if set will perform Open Speedshop profiling. The type of the profiling can'
echo '                   specified using the keyword OPENSS_TYPE. The default type is osspcsamp profiling'
echo '     OPENSS_TYPE : takes effect only if OPENSS_PROF is set.'
echo '                   valid values are osspcsamp, ossusertime, osshwc, osshwctime, ossio, ossiot, ossmpi, ossmpit, ossmpiiotf and ossfpe'
echo '      PSRUN_PROF : if set to anything will perform a psrun profiling and will pick the psrun'
echo '                    installation by loading the perfmon_tools/1.4.7a module from /sw/com/modulefiles'
echo ' PERFCATCHER_PROF : if set to anything will perform perfcatcher profiling and assumes that '
echo '                    perfcatcher is installed under /opt/sgi/perfcather'
echo '   MPINSIDE_PROF : when set to a type will perform MPInside profile and will generate a'
echo '                   MPINSIDE_MAT_DIR directory with mpiplace.file for a later mpiplace job'
echo '                    Takes one of the following values:'
echo " "
echo '                    "basic", "slt", "lst_b" , "perfect" , "perfect_b" or "collectivewait" '
echo '   CLK_SPD       : cpu clock speed being MPInside '
echo ' '
echo '   MPIPLACE_TASK : if set to anything will invoke 'mpiplace', generates a mpiplace_perm file'
echo '                   and runs the solver ufter setting the env var MPI_WORLD_MAP=mpiplace_perm'
echo    
echo ' 1. If performing decomposition only:'
echo
echo '    DECOMP=1,NPROC=<no-domains>,PRECISION=[DP|SP],SCRATCH_DIR=<path>'
echo
echo '    where SCRATCH_DIR is optional else PBS working directory is used'
echo ''
echo '    Example:'
echo
echo '    qsub -l select=1:ncpus=32:mpiprocs=8 -v DECOMP=1,NPROC=256,PRECISION=DP script'
echo
echo '    or specify scratch directory'
echo
echo '    qsub -l select=1:ncpus=32:mpiprocs=8 \\ '
echo '         -v DECOMP=1,NPROC=256,PRECISION=DP,SCRATCH_DIR=/lustre/ayad/OpenFOAM/MyCase script'
echo
echo ' 2. If performing decomposition followed by Solver:'
echo
echo '    DECOMP=2,NPROC=<no-domains>,PRECISION=[DP|SP],MPITYPE=[SGIMPI|HMPT|OPENMPI|HPCX_OPENMPI|INTELMPI],EXE=name,SCRATCH_DIR=<path>,CORES=<range>,MACH=<spec>'
echo
echo '   where '
echo
echo '      SCRATCH_DIR is optional else PBS working directory is used'
echo '      CORES : range of cores, only for SGIMPI, e.g CORES="0-23"'
echo '      CORES keyword is not required if running on SGI UV systems'
echo
echo '   Example: 96 MPI task job, using 4 Compute nodes with 48 virtual cores/node and 24 mpi tasks/node'
echo
echo '   qsub -l select=4:ncpus=48:mpiprocs=24 \\  '
echo '        -v NPROC=96,DECOMP=2,MPITYPE=SGIMPI,EXE=simpleFoam,CORES="0-23",MACH=ICE-X-HASWEL2600 \\  '
echo '        ./script'
echo
echo '   or specify scratch directory'
echo
echo '    qsub -l select=4:ncpus=48:mpiprocs=24 \\  '
echo '         -v NPROC=96,DECOMP=2,MPITYPE=SGIMPI,EXE=simpleFoam,SCRATCH_DIR=/usr2/MYCase,CORES=0-23,MACH=ICE-X-HASWEL2600 \\  '
echo '        script'
echo
echo '   SGI UV example with Hyper-Threading disabled:'
echo
echo '   qsub -l select=1:ncpus=256:mpiprocs=256 \\  '
echo '        -v NPROC=256,DECOMP=2,MPITYPE=SGIMPI,EXE=simpleFoam,SCRATCH_DIR=/usr2/MYCase,MACH=UV2000 \\  '
echo '        -l place=excl:group=iru script'
echo
echo '   SGI UV example with Hyper-Threading enabled:'
echo ' '
echo '   qsub -l select=1:ncpus=512:mpiprocs=256 \\  '
echo '        -v NPROC=256,DECOMP=2,MPITYPE=SGIMPI,EXE=simpleFoam,SCRATCH_DIR=/usr2/MYCase,MACH=UV2000 \\  '
echo "        -l place=excl:group=iru script"
echo
echo ' 3. If performing Solver only, with no decomposition, Decomposition assumed done previously'
echo '    As in 2 above  but set DECOMP=0 '
echo
echo ' Note: If running a job as a Hyper-Threaded job then make sure that 'mpiprocs' is set equal to 'ncpus''
echo '       and that NPROC=ncpus,  if UV, and NPROC=mpiprocsXnoNodes if cluster'
echo 
echo 'Please forward any bug reports or suggestions to Ayad Jassim <ayad@sgi.com>'
echo ''
echo ' 4. Use the DUAL_RAIL=Y to tell the script to use dual rail by sgi MPI on an ICE system, e.g '
echo ''
echo '    qsub -l select=4:ncpus=48:mpiprocs=24 \\  '
echo '         -v NPROC=96,DECOMP=0,MPITYPE=SGIMPI,EXE=simpleFoam,CORES="0-23",MACH=ICE-X-HASWEL2600,DUAL_RAIL=Y \\  '
echo '         script'
echo ' 5. Performing reconstructPar after a solver run'
echo ''
echo '    qsub -l select=1:ncpus=8:mpiprocs=8 -v EXE=reconstructPar,DECOMP=0,NPROC=32 ./script'
echo ''
echo ' 6. Performing reconstructParMesh after a parallel snappyHexMesh run'
echo ''
echo '    qsub -l select=1:ncpus=8:mpiprocs=8 -v EXE=reconstructParMesh,DECOMP=0,NPROC=32 ./script'
echo ''
echo ' 7. Performing SpeedShop hwc profiling on a 448core/16node job:'
echo ''
echo '    qsub -l select=16:ncpus=56:mpiprocs=28:sales_op=none \  '
echo '     -v NPROC=448,DECOMP=0,CORES=0-27,OPENSS_PROF=Y,OPENSS_TYPE=osshwc Ofoam.x '
echo ''
echo ' 8. Performing MPInside basic profiling on a 504core/18 nodes job '
echo '    qsub -l select=18:ncpus=56:mpiprocs=28:sales_op=none -v \ '
echo '    NPROC=504,CORES=0-27,DECOMP=0,MPINSIDE_PROF=basic,CLK_SPD=2600,MACH=OPA_2600,EXE=icoFoam \ '
echo '    -l place=excl:group=iruhalf Ofoam.x.27 '
echo ''
echo ' 9. Running INTELMPI on OPA ICE XA using Daniel Faraj Affinity.sh script '
echo '    qsub -l select=25:ncpus=56:mpiprocs=28:sales_op=none \ '
echo '         -v NPROC=720,MPITYPE=INTELMPI,DECOMP=0,FABRIC=OPA Ofoam.x'
echo ''
echo ''
echo ' 10. Running SYSTEMOPENMPI on OPA ICE XA uisng D Faraj Affinity.sh'
echo '     qsub -l select=25:ncpus=56:mpiprocs=28:sales_op=none \ '
echo '          -v NPROC=700,MPITYPE=SYSTEMOPENMPI,DECOMP=0,FABRIC=OPA Ofoam.x '
echo ''
echo ' 11. Running INTELMPI on ICEXA EDR by selecting equal number (7) cores of each socket per node'
echo '     qsub -l select=21:ncpus=56:mpiprocs=14:sales_op=none+1:ncpus=56:mpiprocs=6:sales_op=none \ '
echo '          -v NPROC=300,MPITYPE=IMPI,PPN=7S14,DECOMP=0,PRECISION=SP Ofoam.x '
echo ''
echo ' 12. Running with MLX HPCX_OPENMPI on EDR cluster with AMD 7702 nodes '
echo '     as Single Precision with taks pinned to every other 4th core including the SMT cores'
echo '     qsub -q f2000THP -l select=4:ncpus=256:mpiprocs=64:sales_op=dl385 -v \ '
echo '       NPROC=256,DECOMP=0,AMD7702=1,MACH=AMD7702_32PHY_32SMT_BY_NODE_THP,PRECISION=SP,BY_NODE=1,MPITYPE=MLX_OPENMPI,CORES=ALT2HT Ofoam.x'
echo ' 13. Running with MLX_OPENMPI on EDR cluster with AMD 7502'
echo '     sbatch -p gre1_all --nodes=4 --constraint="rome,7502" --comment="collectl=no" --ntasks-per-node=64 --export=HOME,ALL,NPROC=256,DECOMP=0,MPITYPE=MLX_OPENMPI,AMD7502=1 ./Ofoam.x'
echo ' Examples on raptor with MLX_OPENMPI:'
echo ' 14. Run blockMesh:'
echo '     sbatch -p rome64  --nodes=1 --ntasks-per-node=64 --export=ALL,EXE=blockMesh ./Ofoam.x'
echo ' 15. Run decompsePar:'
echo '     sbatch -p rome64  --nodes=4 --ntasks-per-node=64 --hint=multithread --export=ALL,NPROC=256,DECOMP=1 Ofoam.x'
echo ' 16. Run checkMesh:'
echo '  sbatch -x "amd76-00[24,26,61-63]" -p rome64  --nodes=4 --ntasks-per-node=64 --hint=multithread --export=ALL,NPROC=256,DECOMP=0,MPITYPE=MLX_OPENMPI,CORES=core,EXE=checkMesh ./Ofoam.x'
echo ' 17. Run patchSummary: '
echo '  sbatch -x "amd76-00[24,26,61-63]" -p rome64  --nodes=4 --ntasks-per-node=64 --hint=multithread --export=ALL,NPROC=256,DECOMP=0,MPITYPE=MLX_OPENMPI,CORES=socket,EXE=patchSummary ./Ofoam.x'
echo ' 18. Run potentialFoam:'
echo '  sbatch -x "amd76-00[24,26,61-63]" -p rome64  --nodes=4 --ntasks-per-node=64 --hint=multithread --export=ALL,NPROC=256,DECOMP=0,MPITYPE=MLX_OPENMPI,CORES="0-63",EXE=potentialFoam ./Ofoam.x'
echo ' 19. Run simpleFoam:'
echo 'sbatch -x "amd76-00[24,26,61-63]" -p rome64  --nodes=4 --ntasks-per-node=64 --hint=multithread --export=ALL,NPROC=256,DECOMP=0,MPITYPE=MLX_OPENMPI,CORES=core ./Ofoam.x'
echo ' 20. Run simpleFoam with TURBO off: Notice CRAY_SLURM_TURBO_OFF=1'
echo ' '
echo 'sbatch -x "amd76-00[24,26,61-63]" -p rome64  --nodes=4 --ntasks-per-node=64 --hint=multithread --export=ALL,NPROC=256,DECOMP=0,MPITYPE=MLX_OPENMPI,CORES=core,CRAY_SLURM_TURBO_OFF=1 ./Ofoam.x'
echo ' '
echo ' 21. Run simpleFoam (Keyword EXE=simpleFoam) with Mellonax OPENMPI, HPCX_OPENMPI on 16 '
echo '     AMD Rome 7542 nodes (keyword AMD7542=1),  a total of 1024 domains (cores),'
echo '     64 MPI tasks/node pinned to 32 physical cores with their corresponding SMT'
echo '     (Hyperthread) cores spread over every 2 cores over a node (Keyword ALT2HT=1)'
echo '     Decomposition assumed have been previously done (keyword DECOMP=0)'
echo '     Output log file name will be marked with AMD7542_ALT2HT (Keyword MACH="AMD7542_ALT2HT")'
echo ' '
echo "sbatch -p gre1_all --nodes=16 --ntasks-per-node=64 --constraint="rome,7542" --comment="collectl=no" --export=ALL,MPITYPE=MLX_OPENMPI,AMD7542=1,ALT2HT=1,MACH=AMD7542,DECOMP=0,NPROC=1024,MACH="AMD7542_ALT2HT",EXE=simpleFoam ./Ofoam.x"
echo ' '
echo ' 22. Run simpleFoam (Keyword EXE=simpleFoam) using OPENMPI on 16 '
echo '     AMD Rome 7702 nodes (keyword AMD7702=1),  a total of 1024 domains (cores),'
echo '     32 MPI tasks/node pinned to 16 physical cores with their corresponding SMT'
echo '     (Hyperthread) cores spread over every 8 cores over a node (Keyword ALT8HT=1)'
echo '     Decomposition assumed have been previously done (keyword DECOMP=0)'
echo '     Output log file name will be marked with AMD7702_ALT8HT (Keyword MACH="AMD7702_ALT8HT")'
echo '     This job will run inside a sub-directory 1024_d, previously created in example 23.'
echo '     Note that the options --constraint="rome,7542" --comment="collectl=no" are SLURM'
echo '     HPE Grenoble customised settings'
echo ' '
echo ' sbatch -p gre1_all --nodes=16 --ntasks-per-node=32 --export=ALL,MPITYPE=OPENMPI,AMD7702=1,ALT8HT=1,MACH=AMD7702,DECOMP=0,NPROC=1024,EXE=1024 ./Ofoam.x'
echo ' '
echo ' 23. Run a 1024 domains decomposPar domain decomposition and exit (DECOMP=1)'
echo '     The decomposeParDict must be named decomposParDict.1024 prior'
echo '     The decomposition processorNNN resulting sub-directories will be'
echo '     created inside the sub-directory 1024_d'
echo ' '
echo ' sbatch -p gre1_all --nodes=1 --ntasks-per-node=4 --export=ALL,NPROC=1024,DECOMP=1 ./foam.x'
echo ' '
echo ' 24. Run a 256 CRAY-MPICH task simpleFoam on two nodes over a PBS system'
echo ' qsub -q f2450 -l select=2:ncpus=256:mpiprocs=128:cluster=tiber:sales_op=7763 -v NPROC=256,MPITYPE=CRAY,DECOMP=0,AMD7702=1,ALT1=1,MACH=CrayCC,CRAY_MPICH_USE_MPIEXEC=1 ./Ofoam.x'
echo ' '
exit
fi
#
slurm_cleanup()
{
   if [ -n "$CRAY_SLURM_TURBO_OFF" ]; then
    if [ "$SLURM_CLUSTER_NAME" = "raptor" ]; then
      srun -n $SLURM_NTASKS  /global/usr/local/turboboost on
    else
#   if[ "$SLURM_CLUSTER_NAME" = "pinot" -o "$SLURM_CLUSTER_NAME" = "hotlum" ]; then
      srun -n $SLURM_NTASKS /lus/scratch/local/bin/turboboost on
    fi
    echo Turbo Boost at end of job Turned back to On
   fi
   if [ -n "$SLURM_JOBID" ]; then
     rm -f $SLURM_NODEFILE
     echo removing machinefile $SLURM_NODEFILE
   fi
}
#
if [ -n "$SLURM_JOBID" ]; then
 export SLURM_CPU_BIND=none
 if [ "$SLURM_CLUSTER_NAME" = "raptor" ]; then
  if [ -n "$CRAY_SLURM_TURBO_OFF" ]; then
     srun -n $SLURM_NTASKS /global/usr/local/turboboost off
     echo Turbo Boost Turned Off
  else
     srun -n $SLURM_NTASKS /global/usr/local/turboboost on
     echo Turbo Boost Turned On
  fi
 fi
 if [ "$SLURM_CLUSTER_NAME" = "pinot" -o "$SLURM_CLUSTER_NAME" = "hotlum" ]; then
  echo "*** Setting Turbo Boost on/off for pinot  or hotlum cluster"
  if [ -n "$CRAY_SLURM_TURBO_OFF" ]; then
     srun -n $SLURM_NTASKS /lus/scratch/local/bin/turboboost off
     echo Turbo Boost Turned Off
  else
     srun -n $SLURM_NTASKS /lus/scratch/local/bin/turboboost on
     echo Turbo Boost Turned On
  fi
 fi
#fi
 if [  "$SLURM_CLUSTER_NAME" = "grenoble" ]; then
   export PATH="/apps/slurm/custom:$PATH"
 fi
 export SLURM_NODEFILE="`pwd`/machinefile.$$"
 srun -l /bin/hostname | sed '/SLURM/d' | sort -n | awk '{print $2}' | sed '/SLURM/d' > $SLURM_NODEFILE
 echo "*** DBG SLURM_NODEFILE listing:"
 echo "********************************************"
 cat $SLURM_NODEFILE
 echo "********************************************"
 export PBS_O_WORKDIR=$SLURM_SUBMIT_DIR
 export SLURM_CPU_BIND=none
else
 export PBS_O_WORKDIR=$PBS_O_WORKDIR
fi
#
cd $PBS_O_WORKDIR
#
echo "Job started at `date`" > Job_Start_End_Dates_${NPROC}cores_Turbo${TURBO}
#
if [ -z ${SCRATCH_DIR} ]; then
   export SCRATCH_DIR=$PBS_O_WORKDIR
elif [ ! -d ${SCRATCH_DIR} ]; then
   mkdir -p ${SCRATCH_DIR}
   if [ $? = 1 ]; then
      echo "***  Unable to create directory ${SCRATCH_DIR}"
      echo "***  Aborting ..."
      exit
   fi
fi
#
echo "DBG: Testing for FOAM_MODULE_FILE"

if [ -n "$FOAM_MODULE_FILE" ]; then
     module use $HOME/modulefiles
     if [ -f $HOME/modulefiles/$FOAM_MODULE_FILE ]; then
         module load $FOAM_MODULE_FILE
         if [ $? = 1 ]; then
            echo "*** Error: error in loading module file $FOAM_MODULE_FILE"
            echo "***        Exiting ..."
            exit
         fi
         export MPITYPE=$WM_MPLIB
         echo "With FOAM_MODULE_FILE -> PATH=$PATH"
         echo "With FOAM_MODULE_FILE -> LD_LIBRARY_PATH=$LD_LIBRARY_PATH"
     else
         echo "*** Error: module file $FOAM_MODULE_FILE does not exist"
         echo "***        Exiting ..."
         exit
     fi
else
   echo "DBG: Testing for OPENFOAM_D $OPENFOAM_D "
   echo "*** Testing for OPENFOAM_D " > setup.log
   if [ -n "${OPENFOAM_D}" ]; then
      if [ -d $OPENFOAM_D ]; then
         echo "*** OPENFOAM_D=$OPENFOAM_D from qsub command line " >> setup.log
      else
         echo "*** OPENFOAM_D=$OPENFOAM_D taken from qsub command line, does not exist " >> setup.log
         echo "***  Existing ..." >> setup.log
         exit
      fi
   elif [ -n "$APPRC" ]; then
         if [ -f $APPRC ]; then
            . $APPRC
            if [ -d $OPENFOAM_D ]; then
               # Now OPENFOAM_D points to the user OpenFOAM installation directory
               echo "*** OPENFOAM_D=$OPENFOAM_D " >> setup.log
            else
               echo "*** OPENFOAM_D=$OPENFOAM_D does not exist " >> setup.log
               echo "***  Existing ..." >> setup.log
               exit
            fi
         else
            echo "***  $APPRC no such file " >> setup.log
            echo "***  Existing ..." >> setup.log
            exit
         fi
   else
         echo "DBG: Testing OPENFOAM_D_DEFAULT"
         if [ ! -d $OPENFOAM_D_DEFAULT ]; then
            echo "*** $OPENFOAM_D_DEFAULT directory does not exist" >> setup.log
            echo "*** Please check the path for OPENFOAM_D_DEFAULT at the top of this script" >> setup.log
            echo "***  Existing ..." >> setup.log
            exit
         fi
         echo "DBG: seting OPENFOAM_D to OPENFOAM_D_DEFAULT"
         export OPENFOAM_D=$OPENFOAM_D_DEFAULT
         echo "DBG OPENFOAM_D=$OPENFOAM_D_DEFAULT"
         echo "*** Using OPENFOAM_D=$OPENFOAM_D as script default " >> setup.log
         #
     #   export FOAM_INST_DIR=$OPENFOAM_D
         #
         if [ -n "$PRECISION" ]; then
            echo PRECISION is $PRECISION
         else
            export PRECISION=DP
            export WARN_PRECISION=1
            echo PRECISION default is DP
         fi
         export WM_PRECISION_OPTION=${PRECISION}
         #
         if [ -n "$MPITYPE" ]; then
            echo MPITYPE is $MPITYPE
         else
            export MPITYPE=SGIMPI
            export WARN_MPITYPE=1
            echo MPITYPE default is SGIMPI 
         fi
	 if [ "$MPITYPE" = "HPCX" -o "$MPITYPE" = "hpcx" -o "$MPITYPE" = "HPCX_OPENMPI" -o "$MPITYPE" = "hpcx_openmpi" ]; then
            export MPITYPE=HPCX_OPENMPI
         fi
         #
         #
         #
   #     if [ $MPITYPE = "CRAY-MPICH" ]; then
            if [ -f $OPENFOAM_D/setup.sh ]; then
             echo "Source $OPENFOAM_D/setup.sh"
             source $OPENFOAM_D/setup.sh $WM_PRECISION_OPTION $MPITYPE >> setup.log
             echo "FOAM_INST_DIR=$FOAM_INST_DIR  ----- GCC `which gcc`"
	     if [ "$MPITYPE" = "HPCX_OPENMPI" ]; then
		     echo "HPCX_HOME=$HPCX_HOME" >> setup.log
             fi
            else
             echo "$OPENFOAM_D/setup.sh does not exist! You need to create one" >> setup.log
             exit
            fi
   #     else
   #        if [ -f $OPENFOAM_D/setup.csh ]; then
   #         echo "Source $OPENFOAM_D/setup.csh"
   #  csh -c source $OPENFOAM_D/setup.csh $WM_PRECISION_OPTION $MPITYPE >> setup.log
   #        else
   #         echo "$OPENFOAM_D/setup.csh does not exist! You need to create one" >> setup.log
   #  exit
   #        fi
   #     fi
   fi

fi  # End of if test for Module file
#
echo "DBG: FOAM_INST_DIR=$FOAM_INST_DIR"
echo "DBG: which GCC `which gcc`"
echo "DBG: WM_MPLIB=$WM_MPLIB"

#
if [ "$EXE" = "blockMesh" ]; then
   cd $SCRATCH_DIR
   taskset -c 1 blockMesh > blockMesh.out 2>&1
   exit
fi
if [ "$EXE" = "setFields" -a "$NPROC" = "1" ]; then
   cd $SCRATCH_DIR
   taskset -c 1setFields > setFields.out 2>&1
   exit
fi
# 
# snappyHexMesh can either serial or parallel
# if EXE=snappyHexMesh and NPROC is not set
# then snappyHexMesh runs serially using the code below
#
if [ "$EXE" = "snappyHexMesh" -a -z "$NPROC" ]; then
   cd $SCRATCH_DIR
   taskset -c 1 `which snappyHexMesh` -overwrite > snappyHexMesh.out 2>&1
   exit
fi
#
if [ -z "${EXE}" ]; then
   export EXE=simpleFoam
   export WARN_EXE=1
fi
if [ -z ${MACH} ]; then
  export WARN_MACH=1
  if [ $DECOMP = "1" ]; then
     export MACH=DECOMPOSITION
     export EXE=decomposePar
  else
     export MACH=TEST
  fi  
fi
if [ -n "$SERIAL" ]; then
   if [ ! -f `which ${EXE} ` ]; then
      echo "*** ${EXE} does not exist "
      echo "*** Aborting "
      exit
   fi
   cd $SCRATCH_DIR
   echo "*** Performing $EXE serial run " > ${EXE}_Serial_${MACH}.out 
   `which taskset` -c 1 `which $EXE ` -case . >> ${EXE}_Serial_${MACH}.out 2>&1
   exit
fi
#
if [ -n "${DUAL_RAIL}" ]; then
   export MACH=${EXE}_${MACH}_DualRail
else
   export MACH=${EXE}_${MACH}
fi
#
if [ -z ${NPROC} ]; then
   echo "***  Number of MPI Tasks, NPROC, has not been specified"
   echo "***  Aborting ..."
   exit
fi
#
echo "*** Generating LOGFILE name"
if [ -n "$SLURM_JOBID" ]; then
  NUM_NODES=`cat $SLURM_NODEFILE | sort -u`
else
  NUM_NODES=`cat $PBS_NODEFILE | sort -u`
fi
NUM_NODES=`echo $NUM_NODES | wc -w`
#
if [ -n "$COPIES" ]; then
   export LOGFILE=${SCRATCH_DIR}/${NPROC}CORES_${NUM_NODES}NODES_${PRECISION}_${MPITYPE}_${MACH}_${COPIES}COPIES.log
   if [ "$MPITYPE" != "HPCX_OPENMPI" ]; then
      echo "*** Multi Copies is only supported with HPCX OPENMPI, Exiting ..." 
      exit
   fi
else
   export LOGFILE=${SCRATCH_DIR}/${NPROC}CORES_${NUM_NODES}NODES_${PRECISION}_${MPITYPE}_${MACH}.log
fi
#
if [ -n "$SLURM_JOBID" ]; then
   if [ -n "$FIRST_NODE_NUM_CORES" ]; then
    # Use the line below if you want to record the total number of lines in the SLURM NODE file.
    #export NLINES="`wc -l ${HOME}/hostfile-jobid-${SLURM_JOB_ID} | awk '{print $1}'`"
     export ARG1="`echo $SLURM_TASKS_PER_NODE`"
     echo $ARG1 | grep '([A-Z,a-z][0-9])'
     if [ $? = 0 ]; then
        export ARG2="`echo $ARG1 | sed  s/\([A-Z,a-z][0-9]\)//`"
     fi
     echo $ARG1 | grep '([A-Z,a-z][0-9][0-9])'
     if [ $? = 0 ]; then
        export ARG2="`echo $ARG1 | sed  s/\([A-Z,a-z][0-9][0-9]\)//`"
     fi
     echo $ARG1 | grep '([A-Z,a-z][0-9][0-9][0-9])'
     if [ $? = 0 ]; then
        export ARG2="`echo $ARG1 | sed  s/\([A-Z,a-z][0-9][0-9][0-9]\)//`"
     fi
     echo ARG1=$ARG1 ARG2=$ARG2
     NUM_LINES_TO_REMOVE="$(( $ARG2 - $FIRST_NODE_NUM_CORES ))"
     echo NUM_LINES_TO_REMOVE=$NUM_LINES_TO_REMOVE SLURM_TASKS_PER_NODE=$SLURM_TASKS_PER_NODE
     i=1
     while [ $i -le $NUM_LINES_TO_REMOVE ];
     do
      # in the ed below replace 1d with \$d if you want to use the last node for the
      # remainder cores
      ed $SLURM_NODEFILE << MYEOF
1d
w
q
MYEOF
      let i++
     done
   fi
   export PBS_NODEFILE=$SLURM_NODEFILE
   cat $PBS_NODEFILE
fi
#
echo "*** SCRIPT_VERSION=$SCRIPT_VERSION" > ${LOGFILE} 2>&1
if [ -n "$FOAM_COMPILER_DIR" ]; then
   if [ -d $FOAM_COMPILER_DIR ]; then
       COMPILER_FLAGS="`cat $FOAM_COMPILER_DIR/c++Opt | grep "^c++OPT"`"
       echo "***" >> ${LOGFILE} 2>&1
       echo "***" >> ${LOGFILE} 2>&1
       echo "***" >> ${LOGFILE} 2>&1
       echo "*** OpenFOAM Release Compiled with $COMPILER_FLAGS" >> $LOGFILE
       echo "***" >> ${LOGFILE} 2>&1
       echo "***" >> ${LOGFILE} 2>&1
       echo "***" >> ${LOGFILE} 2>&1
   else
       echo "***" >> ${LOGFILE} 2>&1
       echo "***" >> ${LOGFILE} 2>&1
       echo "***" >> ${LOGFILE} 2>&1
       echo "Could not locate list of COMPILER_FLAGS for this build" >> $LOGFILE
       echo "Check the definition of FOAM_COMPILER_DIR in your FOAM_INST_DIR setup.sh script" >> $LOGFILE
       echo "***" >> ${LOGFILE} 2>&1
       echo "***" >> ${LOGFILE} 2>&1
       echo "***" >> ${LOGFILE} 2>&1
   fi
else
   echo "Warning: FOAM_COMPILER_DIR environment variable has not been setup in the setup.sh OpenFOAM installation script" >> $LOGFILE
   echo "it will not be possible to print the compiler flags used for this installation" >> $LOGFILE
fi
echo "***  Using Scratch directory ${SCRATCH_DIR}" >> ${LOGFILE} 2>&1
if [ "${WARN_PRECISION}" = "1" ]; then
   echo "***  Warning: PRECISION has not been defined, defaults to DP" >> ${LOGFILE} 2>&1
fi
if [ "${WARN_MPITYPE}" = "1" ]; then
   echo "***  Warning: MPITYPE has not been defined, defaults to SGIMPI" >> ${LOGFILE} 2>&1
fi
if [ "${WARN_MACH}" = "1" ]; then
   echo "***  MACH has not been defined, defaults to ${MACH}" >> ${LOGFILE} 2>&1
fi
#
if [ ! -d $FOAM_INST_DIR ]; then
     echo "***  Can't find OpenFOAM installation directory" > ${LOGFILE} 2>&1
     echo "***  $FOAM_INST_DIR does not exist" >> ${LOGFILE} 2>&1
     echo "***  exiting ..." >> ${LOGFILE} 2>&1
     exit
fi
#
# unsetenv FOAM_SIGFPE:
#
export EXE_PATH=`which $EXE`
echo "DBG: EXE_PATH=$EXE_PATH" >> ${LOGFILE}
if [ -f "$EXE_PATH" ]; then
     echo "*** OpenFOAM executable full path $EXE_PATH "  >> ${LOGFILE} 2>&1
else
     echo "*** OpenFOAM executable $EXE not found, please check OpenFOAM installation"  >> ${LOGFILE} 2>&1
     echo "***  exiting ..." >> ${LOGFILE} 2>&1
     exit
fi
#
if [ -n "$COPIES" ]; then
   if [ "$COPIES" = "" ]; then
       export COPIES=1
       echo "*** Number of COPIES was not set, default one copy" >> ${LOGFILE}
   else
       echo "*** Job will create or run $COPIES copies of decompositions" >> ${LOGFILE}
   fi
#
   if [ $COPIES = 1 ]; then
         export COPIES_LIST="1"
   elif [ $COPIES = 2 ]; then
         export COPIES_LIST="1 2"
   elif [ $COPIES = 3 ]; then
         export COPIES_LIST="1 2 3"
   elif [ $COPIES = 4 ]; then
         export COPIES_LIST="1 2 3 4"
   elif [ $COPIES = 5 ]; then
         export COPIES_LIST="1 2 3 4 5"
   elif [ $COPIES = 6 ]; then
         export COPIES_LIST="1 2 3 4 5 6"
   elif [ $COPIES = 7 ]; then
         export COPIES_LIST="1 2 3 4 5 6 7"
   elif [ $COPIES = 8 ]; then
         export COPIES_LIST="1 2 3 4 5 6 7 8"
   else
         echo "*** Unsupported number of COPIES, maximum is 8" >> ${LOGFILE}
   fi
   echo "*** Job will use $COPIES copies,  COPIES_LIST=$COPIES_LIST" >> ${LOGFILE}
fi
if [ $DECOMP = "0" ]; then
      if [ -n "${WARN_EXE}" ]; then
         echo "***  Warning: EXE has not been defined, defaults to simpleFoam" >> ${LOGFILE} 2>&1
      fi
      echo "*** dbg: current directory `pwd`" >> ${LOGFILE}
      if [ ! -f system/decomposeParDict.${NPROC} ]; then
           echo "***  Please creat a system/decomposeParDict.${NPROC}" >> ${LOGFILE} 2>&1
           echo "***  and set numberOfSubdomains to $NPROC along with the decomp method " >> ${LOGFILE} 2>&1
           echo "***  existing ..." >> ${LOGFILE} 2>&1
           exit
      fi
      echo "*** Copying system/decomposeParDict.${NPROC} to system/decomposeParDict" >> ${LOGFILE}
      cp system/decomposeParDict.${NPROC} system/decomposeParDict
      echo "***  Job will perform tasks with no decomposition" >> ${LOGFILE}
      echo "***  Decomposition assumed allready made prior to this job" >> ${LOGFILE}
      if [ -n "$COPIES" ]; then
         echo "*** Checking for $COPIES copies sub-directories if they exist" >> ${LOGFILE}
         for i in $COPIES_LIST
         do
           if [ ! -d  ${SCRATCH_DIR}/${NPROC}_d_${i} ]; then
            echo "*** Decomposition sub-directory  ${SCRATCH_DIR}/${NPROC}_d_${i} does not exist, exiting ..." >> ${LOGFILE}
            echo "*** Please run the decomposer to create processor directories for all required copies first" >> ${LOGFILE}
	    exit
           fi
         done
      else
         if [ ! -d ${SCRATCH_DIR}/${NPROC}_d ]; then
            echo "*** Decomposition sub-directory  ${SCRATCH_DIR}/${NPROC}_d does not exist, exiting ..." >> ${LOGFILE}
            echo "*** Please run the decomposer to create processor directories first" >> ${LOGFILE}
            exit
         fi
         cd ${SCRATCH_DIR}/${NPROC}_d
         rm -rf 0 system constant
         ln -s ${PBS_O_WORKDIR}/0 0
         cp -r ${PBS_O_WORKDIR}/system system
         ln -s ${PBS_O_WORKDIR}/constant constant
         if [ ! -f system/decomposeParDict.${NPROC} ]; then
              echo "***  Please creat a system/decomposeParDict.${NPROC}" >> ${LOGFILE}
              echo "***  and set numberOfSubdomains to $NPROC along with the decomp method " >> ${LOGFILE}
              echo "***  existing ..." >> ${LOGFILE}
              exit
         fi
         echo "DBG: Copying system/decomposeParDict.${NPROC} to system/decomposeParDict" >> ${LOGFILE}
         cp system/decomposeParDict.${NPROC} system/decomposeParDict
      fi
elif [ $DECOMP = "1" ]; then
      if [ ! -f ${SCRATCH_DIR}/system/decomposeParDict.${NPROC} ]; then
           echo "***  Please creat a system/decomposeParDict.${NPROC}" >> ${LOGFILE}
           echo "***  existing ..." >> ${LOGFILE}
           exit
      fi

      echo "***  Job will perform Domain Decomposition only ..." >> ${LOGFILE}
      if [ -n "$COPIES" ]; then
         for i in $COPIES_LIST
         do
          if [ "$i" = "1" ]; then
            rm -rf ${SCRATCH_DIR}/${NPROC}_d_1
            echo ${SCRATCH_DIR}/${NPROC}_d  >> ${LOGFILE}
            rm -rf ${SCRATCH_DIR}/${NPROC}_d
            mkdir ${SCRATCH_DIR}/${NPROC}_d
            cd ${SCRATCH_DIR}/${NPROC}_d
         #  rm -rf 0 system constant
            cp -r ${PBS_O_WORKDIR}/system system
            STIME=`grep '^startTime'  system/controlDict | awk '{print $2}' | sed -e s/\;//`
   	 echo STIME=$STIME
            if [ ! -d ${PBS_O_WORKDIR}/${STIME} ]; then
               echo "***  Please ${PBS_O_WORKDIR}/${STIME} does not exist, needed by field transfer" >> ${LOGFILE} 2>&1
               echo "***  exiting ..." >> ${LOGFILE} 2>&1
               slurm_cleanup
               exit
            fi
            ln -s ${PBS_O_WORKDIR}/${STIME} $STIME
            ln -s ${PBS_O_WORKDIR}/constant constant
            cp system/decomposeParDict.${NPROC} system/decomposeParDict
            if [ -n "$CPFILES" ]; then
               echo CPFILE $CPFILES
               for j in `echo $CPFILES`
               do
                  echo "DBG: ${SCRATCH_DIR}/${j} `pwd` "
                  #for k in `ls -d proc*`
                  #do
                   cp -v ${SCRATCH_DIR}/${j} .
                  #    cp -v ${SCRATCH_DIR}/${j} ${k}
                  #done
               done
            fi
         #
         # Now decompose
         #
          taskset -c 1 decomposePar -force -case . >> ${LOGFILE} 2>&1
         #
          echo "*** Makeing tar file for copy 1" >> ${LOGFILE}
          tar cPf ${SCRATCH_DIR}/${NPROC}_d.tar ${SCRATCH_DIR}/${NPROC}_d
          mv ${SCRATCH_DIR}/${NPROC}_d ${SCRATCH_DIR}/${NPROC}_d_1
          cd ..
         else
          echo "*** untaring for copy ${i}" >> ${LOGFILE}
          rm -rf ${SCRATCH_DIR}/${NPROC}_d_${i}
          tar xPf  ${SCRATCH_DIR}/${NPROC}_d.tar
          mv ${SCRATCH_DIR}/${NPROC}_d ${SCRATCH_DIR}/${NPROC}_d_${i}
         fi
        done
        rm -rf ${SCRATCH_DIR}/${NPROC}_d.tar
        echo "*** Domain Decomposition done" >> ${LOGFILE} 2>&1
        slurm_cleanup
        exit
     else
      # Normal standard Domain Decomposition, just a single copy
      rm -rf ${SCRATCH_DIR}/${NPROC}_d
      mkdir ${SCRATCH_DIR}/${NPROC}_d
      cd ${SCRATCH_DIR}/${NPROC}_d
      echo "***  Job will perform Domain Decomposition only ..." >> ${LOGFILE} 2>&1
      rm -rf 0 system constant
      cp -r ${PBS_O_WORKDIR}/system system
      STIME=`grep '^startTime'  system/controlDict | awk '{print $2}' | sed -e s/\;//`
      if [ ! -d ${PBS_O_WORKDIR}/${STIME} ]; then
         echo "***  Please ${PBS_O_WORKDIR}/${STIME} does not exist, needed by field transfer" >> ${LOGFILE}
         echo "***  exiting ..." >> ${LOGFILE}
         slurm_cleanup
         exit
      fi
      ln -s ${PBS_O_WORKDIR}/${STIME} $STIME
      ln -s ${PBS_O_WORKDIR}/constant constant
      cp system/decomposeParDict.${NPROC} system/decomposeParDict
      if [ -n "$CPFILES" ]; then
         echo CPFILE $CPFILES
         for i in `echo $CPFILES`
         do
             echo "DBG: ${SCRATCH_DIR}/${i} `pwd` "
            #for j in `ls -d proc*`
            #do
             cp -v ${SCRATCH_DIR}/${i} .
            #    cp -v ${SCRATCH_DIR}/${i} ${j}
            #done
         done
      fi
      echo "*** Performing domain decomposition ..." >> ${LOGFILE}
      echo "*** taskset -c 1 `which decomposePar` -force -case ." >> ${LOGFILE}
      `which taskset` -c 1 `which decomposePar` -force -case . >> ${LOGFILE}
      echo "*** Domain Decomposition done" >> ${LOGFILE}
      slurm_cleanup
      exit
     fi
else
      echo "***  Unkown DECOMP value please use" >> ${LOGFILE} 2>&1
      echo "***   ---> 0 No Decomposition, Solver only" >> ${LOGFILE} 2>&1
      echo "***   ---> 1 Decomposition Only" >> ${LOGFILE} 2>&1
      echo "***  exiting ..." >> ${LOGFILE} 2>&1
      slurm_cleanup
      exit
fi
if [ $EXE = "reconstructParMesh" -o $EXE = "reconstructPar" ]; then
      echo "*** Running $EXE ..." >> ${LOGFILE} 2>&1
      for i in 1 2 3 4
      do
         cd ${SCRATCH_DIR}/${NPROC}_d_${i}
         taskset -c 1 `which $EXE ` >> ${LOGFILE} 2>&1
      done
      slurm_cleanup
      exit
fi
if [ -d "/proc/sgi_uv" -o -n "$USE_DPLACE" ]; then
      PLACE_TOOL=dplace
else
      PLACE_TOOL=taskset
fi
if [ -z "$EXE" ]; then
     echo "***  qsub command does not specify an OpenFOAM executable" >> ${LOGFILE} 2>&1
     echo "***  Please specify an OpenFOAM executable with the EXE=name keyword" >> ${LOGFILE} 2>&1
     echo "***  exiting ..." >> ${LOGFILE} 2>&1
     slurm_cleanup
     exit
fi
#
if [ "$EXE" = "reconstructPar" ]; then
   echo "***  Running reconstructPar " >> ${LOGFILE} 2>&1
   $PLACE_TOOL -c 1 reconstructPar -case . >> ${EXE}_${LOGFILE} 2>&1
   slurm_cleanup
   exit
fi
if [ "$EXE" = "reconstructParMesh" ]; then
   echo "***  Running reconstructParMesh " >> ${LOGFILE} 2>&1
   $PLACE_TOOL -c 1 reconstructParMesh -case . >> ${LOGFILE} 2>&1
   slurm_cleanup
   exit
fi
#
export AFIN=""
#
if [ -z "$FOAM_OPTIONS" ]; then
   export FOAM_OPTIONS=""
fi

echo "Testing for MPITYPE, Pinning sections" >> ${LOGFILE}
if [ "$WM_MPLIB" = "SGIMPI" -o "$WM_MPLIB" = "HMPTMPI" ]; then

     if [ -z "$MPTPIN" ]; then
       if [ -n "$CORES1" -a -n "$CORES2" ]; then
           if [ -n "$CORES3" -a -n "$CORES4" ]; then
             export CORES="$CORES1,$CORES2,$CORES3,$CORES4"
             if [ -n "$CORES5" -a -n "$CORES6" -a -n "$CORES7" -a -n "$CORES8" ]; then
                  export CORES="$CORES1,$CORES2,$CORES3,$CORES4,$CORES5,$CORES6,$CORES7,$CORES8"
             fi
           else
             export CORES="$CORES1,$CORES2"
           fi
       elif [ -n "${AMD9454ALT3HT}" ]; then
           export CORES="0,3,6,9,12,15,18,21,24,27,30,33,36,39,42,45,48,51,54,57,60,63,66,69,72,75,78,81,84,87,90,93,96,99,102,105,108,111,114,117,120,123,126,129,132,135,138,141,144,147,150,153,156,159,162,165,168,171,174,177,180,183,186,189"
       elif [ -n "${AMD9654ALT6HT}" ]; then
           export CORES="0,6,12,18,24,30,36,42,48,54,60,66,72,78,84,90,96,102,108,114,120,126,132,138,144,150,156,162,168,174,180,186,192,198,204,210,216,222,228,234,240,246,252,258,264,270,276,282,288,294,300,306,312,318,324,330,336,342,348,354,360,366,372,378"
       elif [ -n "${AMD7600}" -o -n "${AMD7F52}" -o -n "${AMDROME16}" ]; then
           export AMD=1
           if [ -z "$ALT1" -a -z "$ALT2" -a -z "$ALT4" -a -z "$ALT8" -a -z "$ALT1HT" -a -z "$ALT2HT" -a -z "$ALT4HT" -a -z "$ALT8HT" ]; then
              echo "An AMD_ROME16 was selected but no valid ALT value given" >> $LOGFILE
              echo "Please set any of ALT1, ALT2, ALT4,ALT8, ALT1HT, ALT2HT or ALT8HT" >> $LOGFILE
              echo "Exiting ..." >> $LOGFILE
              exit
           elif [ -n "$ALT1" ]; then
               # Using 32 physical cores per nodes
               export CORES="0-31"
           elif [ -n "$ALT1HT" ]; then
               export CORES="0-63"
           elif [ -n "$ALT2" ]; then
               # Using 16 physical cores per nodes
              export CORES="0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30"
           elif [ -n "$ALT2HT" ]; then
              export CORES="0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62"
           elif [ -n "$ALT4" ]; then
               # Using 8 physical cores per nodes
              export CORES="0,4,8,12,16,20,24,28"
           elif [ -n "$ALT4HT" ]; then
              export CORES="0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60"
           elif [ -n "$ALT8" ]; then
               # Using 4 physical cores per nodes
              export CORES="0,8,16,24"
           elif [ -n "$ALT8HT" ]; then
              export CORES="0,8,16,24,32,40,48,56"
           fi
       elif [ -n "${AMD7543}" -o -n "${AMD7502}" -o -n "${AMD7601}" -o -n "${AMD7532}" -o -n "${AMD7542}" ]; then
           if [ -z "$ALT1" -a -z "$ALT1HT" -a -z "$ALT2" -a -z "$ALT2HT" -a -z "$ALT4" -a -z "$ALT4HT" -a -z "$ALT8" -a -z "$ALT8HT" ]; then
              echo "Warning: An ALT pin value was not supplied or incorrect" >> $LOGFILE
              echo "Use: ALT1, ALT1HT, ALT2, ALT2HT, ALT4, ALT4HT, ALT8, ALT8HT" >> $LOGFILE
              echo "the integer in the ALTx is the number of cores to alternate between pinnings" >> $LOGFILE
              echo "Will use an ALT1 value to pin to cores 0 to 127 sequentially" >> $LOGFILE
              export ALT1=1
           fi
           if [ -n "$ALT1" ]; then
               export CORES="0-63"
           elif [ -n "$ALT1HT" ]; then
               export CORES="0-127"
           elif [ -n "$ALT2" ]; then
              # Using 32 core per node
              export CORES="0-63/2"
           elif [ -n "$ALT2HT" ]; then
              # Using 32 core per node
              # export CORES="0,64,2,66,4,68,6,70,8,72,10,74,12,76,14,78,16,80,18,82,20,84,22,86,24,88,26,90,28,92,30,94,32,96,34,98,36,100,38,102,40,104,42,106,44,108,46,110,48,112,50,114,52,116,54,118,56,120,58,122,60,124,62,126"
              export CORES="0-127/2"
           elif [ -n "$ALT4" ]; then
              # Using 16 core per node
              export CORES="0-63/4"
           elif [ -n "$ALT4HT" ]; then
              export CORES="0-127/4"
           elif [ -n "$ALT8" ]; then
              # Using 8 core per node
              export CORES="0-63/8"
           elif [ -n "$ALT8HT" ]; then
              export CORES="0-127/8"
           fi
           elif [ -n "${AMD75F3_24c}" ]; then
              echo "DBG: Using AMD75F3_24c emulation"
              export CORES="1,2,3,4,6,7,8,9,12,13,14,15,17,18,19,20,23,24,25,26,28,29,30,31,33,34,35,36,38,39,40,41,44,45,46,47,48,49,50,51,53,54,55,56,59,60,61,63"
       elif [ -n "${AMD7702}" -o -n "${AMD7742}" ]; then
           echo "DBG: AMD7702=1" >> $LOGFILE
           if [ -z "$ALT1" -a -z "$ALT1HT" -a -z "$ALT2" -a -z "$ALT2HT" -a -z "$ALT3" -a -z "$ALT4" -a -z "$ALT4HT" -a -z "$ALT8" -a -z "$ALT8HT" -a -z "$ALT16" -a -z "$ALT16HT" -a -z "$ALT16HT" -a -z "$ALT4MBcashPCore" -a -z "$ALT8MBcashPCore" -a -z "$ALT96" ]; then
              echo "Warning: An ALT pin value was not supplied or incorrect" >> $LOGFILE
              echo "Use: ALT1, ALT1HT, ALT2, ALT2HT, ALT4, ALT4HT, ALT8, ALT8HT, ALT16, ALT16HT" >> $LOGFILE
              echo "the integer in the ALTx is the number of cores to alternate between pinnings" >> $LOGFILE
              echo "Will use an ALT1 value to pin to cores 0 to 127 sequentially" >> $LOGFILE
              export ALT1=1
           fi
           if [ -n "$ALT1" ]; then
               export CORES="0-127"
           elif [ -n "$ALT1HT" ]; then
               export CORES="0-255"
           elif [ -n "$ALT2" ]; then
              export CORES="0-127/2"
           elif [ -n "$ALT2HT" ]; then
              #export CORES="0,128,2,130,4,132,6,134,8,136,10,138,12,140,14,142,16,144,18,146,20,148,22,150,24,152,26,154,28,156,30,158,32,160,34,162,36,164,38,166,40,168,42,170,44,172,46,174,48,176,50,178,52,180,54,182,56,184,58,186,60,188,62,190,64,192,66,194,68,196,70,198,72,200,74,202,76,204,78,206,80,208,82,210,84,212,86,214,88,216,90,218,92,220,94,222,96,224,98,226,100,228,102,230,104,232,106,234,108,236,110,238,112,240,114,242,116,244,118,246,120,248,122,250,124,252,126,254"
              export CORES="0-255/2"
           elif [ -n "$ALT3" ]; then
              export CORES="0-2,4-6,8-10,12-14,16-18,20-22,24-26,28-30,32-34,36-38,40-42,44-46,48-50,52-54,56-58,60-62,64-66,68-70,72-74,76-78,80-82,84-86,88-90,92-94,96-98,100-102,104-106,108-110,112-114,116-118,120-122,124-126"
              echo "DBG: ALT3 selected"
           elif [ -n "$ALT4" ]; then
              export CORES="0-127/4"
           elif [ -n "$ALT4HT" ]; then
              #export CORES="0,128,4,132,8,136,12,140,16,144,20,148,24,152,28,156,32,160,36,164,40,168,44,172,48,176,52,180,56,184,60,188,64,192,68,196,72,200,76,204,80,208,84,212,88,214,92,218,96,222,100,226,104,230,108,236,112,240,116,244,120,248,124,252"
              export CORES="0-255/4"
           elif [ -n "$ALT8" ]; then
              export CORES="0-127/8"
           #  export CORES="1,9,17,25,33,41,49,57,65,73,81,89,97,105,113,121"
           elif [ -n "$ALT8HT" ]; then
             #export CORES="0,128,8,136,16,144,24,152,32,160,40,168,48,176,56,184,64,192,72,200,80,208,88,216,96,224,104,232,112,240,120,248"
              export CORES="0-255/8"
           elif [ -n "$ALT16" ]; then
              export CORES="0-127/16"
           elif [ -n "$ALT16HT" ]; then
              export CORES="0-255/16"
           elif [ -n "$ALT4MBcashPCore" ]; then
             echo "Using ALT4MBcashPCore pinning for AMD7702/7742 ..." >> $LOGFILE
             export CORES="0,1,2,3,4,5,6,7,16,17,18,19,20,21,22,23,32,33,34,35,36,37,38,39,48,49,50,51,52,53,54,55,56,64,65,66,67,68,69,70,71,80,81,82,83,84,85,86,87,96,97,98,99,100,101,102,103,112,113,114,115,116,117,118,119"
           elif [ -n "$ALT8MBcashPCore" ]; then
             echo "Using ALT8MBcashPCore pinning for AMD7702/7742 ..." >> $LOGFILE
             export CORES="0,1,4,5,8,9,12,13,16,17,20,21,24,25,28,29,32,33,36,37,40,41,44,45,48,49,52,53,56,57,60,61,64,65,68,69,72,73,76,77,80,81,84,85,88,89,92,93,96,97,100,101,104,105,108,109,112,113,116,117,120,121,124,125"
           elif [ -n "$ALT96" ]; then
             export CORES="0,1,2,4,5,6,8,9,10,12,13,14,16,17,18,20,21,22,24,25,26,28,29,30,32,33,34,36,37,38,40,41,42,44,45,46,48,49,50,52,53,54,56,57,58,60,61,62,64,65,66,68,69,70,72,73,74,76,77,78,80,81,82,84,85,86,88,89,90,92,93,94,96,97,98,100,101,102,104,105,106,108,109,110,112,113,114,116,117,118,120,121,122,124,125,126"
           elif [ -n "$ALT96" ]; then
             export CORES="0,1,2,4,5,6,8,9,10,12,13,14,16,17,18,20,21,22,24,25,26,28,29,30,32,33,34,36,37,38,40,41,42,44,45,46,48,49,50,52,53,54,56,57,58,60,61,62,64,65,66,68,69,70,72,73,74,76,77,78,80,81,82,84,85,86,88,89,90,92,93,94,96,97,98,100,101,102,104,105,106,108,109,110,112,113,114,116,117,118,120,121,122,124,125,126"
           fi
       elif [ -z "$CORES" ];then
             export CORES="0-35"
             echo "*** Warning: CORES was not set, defaults to 0-35" >> $LOGFILE
       fi  # Closing fi for CORESS tests
       export MPI_DSM_CPULIST="${CORES}:allhosts"
     fi # Closing fi for MPTPIN test

elif [ "$WM_MPLIB" = "CRAY-MPICH" ]; then

      if [ -z "$CORES" -a -z "${AMD7600}" -a -z "${AMD7F42}" -a -z "${AMDROME16}" -a -z "${AMD7543}" -a -z "${AMD7502}" -a -z "${AMD7532}" -a -z "${AMD7601}" -a -z "${AMD7542}" -a -z "${AMD7702}" -a -z "${AMD7742}" -a -z "$ALT96" ];then
          if [ -n "${USER_PPN}" ]; then
             export CORES="${USER_PPN}"
          fi
      elif [ -n "$CORES"  ]; then
             export CORES="`seq -s, 1 127`"
	     echo "*** CORES="$CORES" set internally" >> $LOGFILE
      elif [ -n "${AMD7600}" -o -n "${AMD7F52}" -o -n "${AMDROME16}" ]; then
           export AMD=1
           if [ -z "$ALT1" -a -z "$ALT2" -a -z "$ALT4" -a -z "$ALT8" -a -z "$ALT1HT" -a -z "$ALT2HT" -a -z "$ALT4HT" -a -z "$ALT8HT" ]; then
              echo "An AMD_ROME16 was selected but no valid ALT value given" >> $LOGFILE
              echo "Please set any of ALT1, ALT2, ALT4,ALT8, ALT1HT, ALT2HT or ALT8HT" >> $LOGFILE
              echo "Exiting ..." >> $LOGFILE
              exit 
           elif [ -n "$ALT1" ]; then
               # Using 32 physical cores per nodes
               export CORES="`seq -s, 0 31`"
           elif [ -n "$ALT1HT" ]; then
               export CORES="`seq -s, 0 63`"
           elif [ -n "$ALT2" ]; then
               # Using 16 physical cores per nodes
              export CORES="`seq -s, 0 2 31`"
           elif [ -n "$ALT2HT" ]; then
              export CORES="`seq -s, 0 2 63`"
           elif [ -n "$ALT4" ]; then
               # Using 8 physical cores per nodes
              export CORES="`seq -s, 0 4 31`"
           elif [ -n "$ALT4HT" ]; then
              export CORES="`seq -s, 0 4 63`"
           elif [ -n "$ALT8" ]; then
               # Using 4 physical cores per nodes
              export CORES="`seq -s, 0 8 31`"
           elif [ -n "$ALT8HT" ]; then
              export CORES="`seq -s, 0 8 63`"
           fi
      elif [ -n "${AMD7543}" -o -n "${AMD7502}" -o -n "${AMD7532}" -o -n "${AMD7601}" -o -n "${AMD7542}" ];then
           export AMD=1
           if [ -z "$ALT1" -a -z "$ALT2" -a -z "$ALT4" -a -z "$ALT8" -a -z "$ALT1HT" -a -z "$ALT2HT" -a -z "$ALT4HT" -a -z "$ALT8HT" ]; then
               export CORES="`seq -s, 0 63`"
           elif [ -n "$ALT1" ]; then
               export CORES="`seq -s, 0 63`"
           elif [ -n "$ALT1HT" ]; then
               export CORES="`seq -s, 0 127`"
           elif [ -n "$ALT2" ]; then
               export CORES="`seq -s, 0 2 63`"
           elif [ -n "$ALT2HT" ]; then
               export CORES="`seq -s, 0 2 127`"
           elif [ -n "$ALT4" ]; then
               export CORES="`seq -s, 0 4 63`"
           elif [ -n "$ALT4HT" ]; then
               export CORES="`seq -s, 0 4 127`"
           elif [ -n "$ALT8" ]; then
               export CORES="`seq -s, 0 8 63`"
           elif [ -n "$ALT8HT" ]; then
               export CORES="`seq -s, 0 8 127`"
           else
              echo Unkown CORES or ALTx AMD EPYC 7543, 7502, 7532, 7601 and 7542 value , exit >> $LOGFILE
              exit
           fi
      elif [ -n "${AMD75F3_24c}" ]; then
              echo "DBG: Using AMD75F3_24c emulation"
              export CORES="1,2,3,4,6,7,8,9,12,13,14,15,17,18,19,20,23,24,25,26,28,29,30,31,33,34,35,36,38,39,40,41,44,45,46,47,48,49,50,51,53,54,55,56,59,60,61,63"
      elif [ -n "${AMD7702}" -o -n "${AMD7742}" ]; then
           export AMD=1
           if [ -z "$ALT1" -a -z "$ALT2" -a -z "$ALT4" -a -z "$ALT8" -a -z "$ALT16"  -a -z "$ALT1HT" -a -z "$ALT2HT" -a -z "$ALT4HT" -a -z "$ALT8HT" -a -z "$ALT16HT" -a -z "$ALT96" ]; then
               export CORES="`seq -s, 0 127`"
	   elif [ -n "$ALT1" ]; then
               export CORES="`seq -s, 0 127`"
           elif [ -n "$ALT1HT" ]; then
               export CORES="`seq -s, 0 255`"
           elif [ -n "$ALT2" ]; then
               export CORES="`seq -s, 0 2 127`"
           elif [ -n "$ALT2HT" ]; then
               export CORES="`seq -s, 0 2 255`"
           elif [ -n "$ALT4" ]; then
               export CORES="`seq -s, 0 4 127`"
           elif [ -n "$ALT4HT" ]; then
               export CORES="`seq -s, 0 4 255`"
           elif [ -n "$ALT8" ]; then
               export CORES="`seq -s, 0 8 127`"
           elif [ -n "$ALT8HT" ]; then
               export CORES="`seq -s, 0 8 255`"
           elif [ -n "$ALT16" ]; then
               export CORES="`seq -s, 0 16 127`"
           elif [ -n "$ALT16HT" ]; then
               export CORES="`seq -s, 0 16 255`"
           elif [ -n "$ALT96" ]; then
             #skip every 3rd core to emulate 96 cores
             export CORES="0,1,2,4,5,6,8,9,10,12,13,14,16,17,18,20,21,22,24,25,26,28,29,30,32,33,34,36,37,38,40,41,42,44,45,46,48,49,50,52,53,54,56,57,58,60,61,62,64,65,66,68,69,70,72,73,74,76,77,78,80,81,82,84,85,86,88,89,90,92,93,94,96,97,98,100,101,102,104,105,106,108,109,110,112,113,114,116,117,118,120,121,122,124,125,126 "
           else
              echo "Unkown CORES AMD EPYC 7702 value $CORES, exit"
              exit
           fi
      else
           echo "Unkown or ambigious pin selection, use -help option for possible pinning options"
           echo "Exiting ..."	   
           exit
      fi
      echo "CORES=$CORES" >> ${LOGFILE} 2>&1

elif [ "$WM_MPLIB" = "OPENMPI" -o "$WM_MPLIB" = "HPCX_OPENMPI" ]; then

    echo "*** Job Uses $WM_MPLIB ..." >> $LOGFILE
    echo "*** Testing for AMD7502 if set " >> $LOGFILE
    if [ -n "$AMD9454ALT3HT" ]; then
            seq_str="`seq 0 3 191`"
            export CORES_LIST="`echo $seq_str`"
            export ALT=3
    elif [ -n "$AMD9654ALT6HT" ]; then
            seq_str="`seq 0 6 383`"
            export CORES_LIST="`echo $seq_str`"
            echo CORES_LIST="$CORES_LIST"  >> ${LOGFILE}
            export ALT=6
    elif [ -n "${AMD7502}" -o -n "${AMD7601}" -o -n "${AMD7532}" -o -n "${AMD7542}" ]; then
           echo "*** Pinning section AMD7502" >> $LOGFILE
           if [ -z "$ALT1" -a -z "$ALT1HT" -a -z "$ALT2" -a -z "$ALT2HT" -a -z "$ALT4" -a -z "$ALT4HT" -a -z "$ALT8" -a -z "$ALT8HT" ]; then
              echo "Warning: An ALT pin value was not supplied or incorrect" >> $LOGFILE
              echo "Use: ALT1, ALT1HT, ALT2, ALT2HT, ALT4, ALT4HT, ALT8, ALT8HT" >> $LOGFILE
              echo "the integer in the ALTx is the number of cores to alternate between pinnings" >> $LOGFILE
              echo "Will use an ALT1 value to pin to cores 0 to 63 sequentially" >> $LOGFILE
              export ALT1=1
	      export ALT=$ALT1
           fi
           if [ -n "$ALT1" ]; then
               echo "*** Pinning section AMD702 -> ALT1=1" >> $LOGFILE
	       seq_str="`seq 0 63`"
	       export CORES_LIST="`echo $seq_str`"
               export ALT=1
           elif [ -n "$ALT1HT" ]; then
              export ALT=1
	      seq_str="`seq 0 127`"
               export CORES_LIST="`echo $seq_str`"
           elif [ -n "$ALT2" ]; then
              # Using 32 core per node
              seq_str="`seq 0 2 63`"
              export CORES_LIST="`echo $seq_str`"
              export ALT=2
           elif [ -n "$ALT2HT" ]; then
              # Using 32 core per node
              seq_str="`seq 0 2 127`"
              export CORES_LIST="`echo $seq_str`"
              export ALT=2
           elif [ -n "$ALT4" ]; then
              # Using 16 core per node
              seq_str="`seq 0 4 63`"
              export CORES_LIST="`echo $seq_str`"
              export ALT=4
           elif [ -n "$ALT4HT" ]; then
              seq_str="`seq 0 4 127`"
              export CORES_LIST="`echo $seq_str`"
              export ALT=4
           elif [ -n "$ALT8" ]; then
              # Using 8 core per node
	      seq_str="`seq 0 8 63`"
              export ALT=8
           elif [ -n "$ALT8HT" ]; then
              # Using 8 core per node
              seq_str="`seq 0 8 127`"
              export CORES_LIST="`echo $seq_str`"
              export ALT=8
           fi
    elif [ -n "${AMD75F3_24c}" ]; then
           echo "DBG: Using AMD75F3_24c emulation"
           export CORES_LIST="1 2 3 4 6 7 8 9 12 13 14 15 17 18 19 20 23 24 25 26 28 29 30 31 33 34 35 36 38 39 40 41 44 45 46 47 48 49 50 51 53 54 55 56 59 60 61 63"
    elif [ -n "${AMD7702}" -o -n "${AMD7742}" -o -n "${AMD7763}" ]; then
           if [ -z "$ALT1" -a -z "$ALT1HT" -a -z "$ALT2" -a -z "$ALT2HT" -a -z "$ALT4" -a -z "$ALT4HT" -a -z "$ALT8" -a -z "$ALT8HT" -a -z "$ALT16" -a -z "$ALT16HT" -a -z "$ALT32" -a -z "$ALT32HT" -a -z "$ALT_HWTHREAD_CPUS" -a -z "$ALT4MBcashPCore" -a -z "$ALT8MBcashPCore" -a -z "$AMD96ALT1" -a -z "$AMD96ALT1HT" -a -z "$AMD96ALT2" -a -z "$AMD96ALT2HT" -a -z "$AMD96ALT4" -a -z "$AMD96ALT4HT" -a -z "$AMD96ALT8" -a -z "$AMD96ALT8HT" -a -z "$AMD96ALT16" -a -z "$AMD96ALT16HT" ]; then
              echo "Warning: An ALT pin value was not supplied or incorrect" >> $LOGFILE
              echo "Use: ALT1, ALT1HT, ALT2, ALT2HT, ALT4, ALT4HT, ALT8, ALT8HT, ALT16, ALT16HT" >> $LOGFILE
              echo "the integer in the ALTx is the number of cores to alternate between pinnings" >> $LOGFILE
              echo "Will use an ALT1 value to pin to cores 0 to 127 sequentially" >> $LOGFILE
              export ALT1=1
	      export ALT=$ALT1
           fi
           if [ -n "$ALT1" ]; then
	       seq_str="`seq 0 127`"
	       export CORES_LIST="`echo $seq_str`"
               export ALT=1
           elif [ -n "$ALT1HT" ]; then
	       seq_str="`seq 0 255`"
	       export CORES_LIST="`echo $seq_str`"
               export ALT=1
           elif [ -n "$ALT2" ]; then
	      seq_str="`seq 0 2 127`"
	      export CORES_LIST="`echo $seq_str`"
              export ALT=2
           elif [ -n "$ALT2HT" ]; then
	      seq_str="`seq 0 2 255`"
              export CORES_LIST="`echo $seq_str`"
              export ALT=2
           elif [ -n "$ALT4" ]; then
	      seq_str="`seq 0 4 127`"
              export CORES_LIST="`echo $seq_str`"
              export ALT=4
           elif [ -n "$ALT4HT" ]; then
	      seq_str="`seq 0 4 255`"
              export CORES_LIST="`echo $seq_str`"
              export ALT=4
           elif [ -n "$ALT8" ]; then
	      seq_str="`seq 0 8 127`"
              export CORES_LIST="`echo $seq_str`"
              export ALT=8
           elif [ -n "$ALT8HT" ]; then
	      seq_str="`seq 0 8 255`"
              export CORES_LIST="`echo $seq_str`"
              export ALT=8
           elif [ -n "$ALT16" ]; then
	      seq_str="`seq 0 16 127`"
              export CORES_LIST="`echo $seq_str`"
              export ALT=16
           elif [ -n "$ALT16HT" ]; then
	      seq_str="`seq 0 16 255`"
              export CORES_LIST="`echo $seq_str`"
              export ALT=16
           elif [ -n "$ALT32" ]; then
              export CORES_LIST="0 32 64 96"
	      seq_str="`seq 0 32 127`"
              export CORES_LIST="`echo $seq_str`"
	      export ALT=32
           elif [ -n "$ALT32HT" ]; then
	      seq_str="`seq 0 32 255`"
              export CORES_LIST="`echo $seq_str`"
	      export ALT=32
           elif [ -n "$ALT_HWTHREAD_CPUS" ]; then
                     CORES_LIST="0 1 8 9 16 17 24 25 32 33 40 41 48 49 56 57 64 65 72 73 80 81 88 89 96 97 104 105 112 113 120 121"
           elif [ -n "$ALT4MBcashPCore" ]; then
             echo "Using ALT4MBcashPCore pinning for AMD7702/7742 ..." >> $LOGFILE
              export CORES_LIST="0 1 2 3 4 5 6 7 16 17 18 19 20 21 22 23 32 33 34 35 36 37 38 39 48 49 50 51 52 53 54 55 56 64 65 66 67 68 69 70 71 80 81 82 83 84 85 86 87 96 97 98 99 100 101 102 103 112 113 114 115 116 117 118 119"
           elif [ -n "$ALT8MBcashPCore" ]; then
             echo "Using ALT8MBcashPCore pinning for AMD7702/7742 ..." >> $LOGFILE
              export CORES_LIST="0 1 4 5 8 9 12 13 16 17 20 21 24 25 28 29 32 33 36 37 40 41 44 45 48 49 52 53 56 57 60 61 64 65 68 69 72 73 76 77 80 81 84 85 88 89 92 93 96 97 100 101 104 105 108 109 112 113 116 117 120 121 124 125"
           elif [ -n "$AMD96ALT1" ]; then
             echo "Pin to every three and skip 4th core" >> $LOGFILE
             export CORES_LIST="0 1 2 4 5 6 8 9 10 12 13 14 16 17 18 20 21 22 24 25 26 28 29 30 32 33 34 36 37 38 40 41 42 44 45 46 48 49 50 52 53 54 56 57 58 60 61 62 64 65 66 68 69 70 72 73 74 76 77 78 80 81 82 84 85 86 88 89 90 92 93 94 96 97 98 100 101 102 104 105 106 108 109 110 112 113 114 116 117 118 120 121 122 124 125 126"
           elif [ -n "$AMD96ALT1HT" ]; then
              echo "Pin to every three and skip 4th core" >> $LOGFILE
	      export CORES_LIST="0 1 2 4 5 6 8 9 10 12 13 14 16 17 18 20 21 22 24 25 26 28 29 30 32 33 34 36 37 38 40 41 42 44 45 46 48 49 50 52 53 54 56 57 58 60 61 62 64 65 66 68 69 70 72 73 74 76 77 78 80 81 82 84 85 86 88 89 90 92 93 94 96 97 98 100 101 102 104 105 106 108 109 110 112 113 114 116 117 118 120 121 122 124 125 126 128 129 130 132 133 134 136 137 138 140 141 142 144 145 146 148 149 150 152 153 154 156 157 158 160 161 162 164 165 166 168 169 170 172 173 174 176 177 178 180 181 182 184 185 186 188 189 190 192 193 194 196 197 198 200 201 202 204 205 206 208 209 210 212 213 214 216 217 218 220 221 222 224 225 226 228 229 230 232 233 234 236 237 238 240 241 242 244 245 246 248 249 250 252 253 254"
           elif [ -n "$AMD96ALT2" ]; then
             echo "Pin to every three and skip 4th core" >> $LOGFILE
             export CORES_LIST="0 2 5 8 10 13 16 18 21 24 26 29 32 34 37 40 42 45 48 50 53 56 58 61 64 66 69 72 74 77 80 82 85 88 90 93 96 98 101 104 106 109 112 114 117 120 122 125"
           elif [ -n "$AMD96ALT2HT" ]; then
             echo "Pin to every three and skip 4th core" >> $LOGFILE
             export CORES_LIST="0 2 5 8 10 13 16 18 21 24 26 29 32 34 37 40 42 45 48 50 53 56 58 61 64 66 69 72 74 77 80 82 85 88 90 93 96 98 101 104 106 109 112 114 117 120 122 125 128 130 133 136 138 141 144 146 149 152 154 157 160 162 165 168 170 173 176 178 181 184 186 189 192 194 197 200 202 205 208 210 213 216 218 221 224 226 229 232 234 237 240 242 245 248 250 253"
           elif [ -n "$AMD96ALT4" ]; then
             echo "Pin to every three and skip 4th core" >> $LOGFILE
             export CORES_LIST="0 5 10 16 21 26 32 37 42 48 53 58 64 69 74 80 85 90 96 101 106 112 117 122"
           elif [ -n "$AMD96ALT4HT" ]; then
             export CORES_LIST="0 5 10 16 21 26 32 37 42 48 53 58 64 69 74 80 85 90 96 101 106 112 117 122 128 133 138 144 149 154 160 165 170 176 181 186 192 197 202 208 213 218 224 229 234 240 245 250"
           elif [ -n "$AMD96ALT8" ]; then
             export CORES_LIST="0 10 21 32 42 53 64 74 85 96 106 117"
           elif [ -n "$AMD96ALT8HT" ]; then
             export CORES_LIST="0 10 21 32 42 53 64 74 85 96 106 117 128 138 149 160 170 181 192 202 213 224 234 245"
           elif [ -n "$AMD96ALT16" ]; then
             export CORES_LIST="0 21 42 64 85 106"
           elif [ -n "$AMD96ALT16HT" ]; then
             export CORES_LIST="0 21 42 64 85 106 128 149 170 192 213 234"
           fi
    elif [ -z "${PPN}" -a -z "${USER_PPN}" ]; then
         export PPN=128
	 export ALT=1
    elif [ "${PPN}" = "256" ]; then
         export CORES_LIST="`seq 0 255`"
	 export ALT=1
    elif [ "${PPN}" = "128" ]; then
         export CORES_LIST="`seq 0 127`"
	 export ALT=1
    elif [ "${PPN}" = "72" ]; then
         export CORES_LIST="`seq 0 71`"
	 export ALT=1
    elif [ "${PPN}" = "88" ]; then
         export CORES_LIST="`seq 0 87`"
	 export ALT=1
    elif [ "${PPN}" = "64" ]; then
         export CORES_LIST="`seq 0 63`"
	 export ALT=1
    elif [ "${PPN}" = "56" ]; then
         export CORES_LIST="`seq 0 55`"
	 export ALT=1
    elif [ "${PPN}" = "48" ]; then
         export CORES_LIST="`seq 0 47`"
	 export ALT=1
    elif [ "${PPN}" = "44" ]; then
         export CORES_LIST="`seq 0 43`"
	 export ALT=1
    elif [ "${PPN}" = "40" ]; then
         export CORES_LIST="`seq 0 39`"
	 export ALT=1
    elif [ "${PPN}" = "36" ]; then
         export CORES_LIST="0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35"
	 export ALT=1
    elif [ "${PPN}" = "32" ]; then
         export CORES_LIST="0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31"
	 export ALT=1
    elif [ "${PPN}" = "28" ]; then
         export CORES_LIST="0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27"
	 export ALT=1
    elif [ "${PPN}" = "24" ]; then
         export CORES_LIST="0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23"
	 export ALT=1
    elif [ "${PPN}" = "20" ]; then
         export CORES_LIST="0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19"
	 export ALT=1
    elif [ "${PPN}" = "16" ]; then
         export CORES_LIST="0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15"
	 export ALT=1
    elif [ "${PPN}" = "12" ]; then
         export CORES_LIST="0 1 2 3 4 5 6 7 8 9 10 11"
	 export ALT=1
    elif [ "${PPN}" = "8" ]; then
         export CORES_LIST="0 1 2 3 4 5 6 7"
	 export ALT=1
    elif [ "${PPN}" = "6" ]; then
         export CORES_LIST="0 1 2 3 4 5"
	 export ALT=1
    elif [ "${PPN}" = "4" ]; then
         export CORES_LIST="0 1 2 3"
	 export ALT=1
    elif [ -n "$CORES" ]; then
        echo "*** CORES=$CORES"  >> $LOGFILE
        export CORES_LIST="seq $CORES"
        export CORES_LIST="`echo $CORES_LIST | sed -e 's/\-/ /'`"
	export ALT=1
    elif [ -n "$USER_PPN" ]; then
         export CORES_LIST=$USER_PPN
         echo "Using USER_PPN defined value" >> $LOGFILE
         echo "USER_PPN=$USER_PPN" >> $LOGFILE
    else
         echo "Unrecognised PPN pin value, use 256, 128, 88, 64, 56, 48, 44, 40, 36, 32, 28, 24, 20, 16, 12, 8, 6 or 4 " >> $LOGFILE
         echo "Alternatively, you may defne your own PPN comma seperated core list using USER_PPN environment variable" >> $LOGFILE
         exit
    fi
elif [ "$WM_MPLIB" = "INTELMPI" ]; then

    export I_MPI_PIN=1

    if [ -n "${AMD7600}" -o -n "${AMD7F52}" -o -n "${AMDROME16}" ]; then
           export AMD=1
           if [ -z "$ALT1" -a -z "$ALT2" -a -z "$ALT4" -a -z "$ALT8" -a -z "$ALT1HT" -a -z "$ALT2HT" -a -z "$ALT4HT" -a -z "$ALT8HT" ]; then
              echo "An AMD_ROME16 was selected but no valid ALT value given" >> $LOGFILE
              echo "Please set any of ALT1, ALT2, ALT4,ALT8, ALT1HT, ALT2HT or ALT8HT" >> $LOGFILE
              echo "Exiting ..." >> $LOGFILE
              exit
           elif [ -n "$ALT1" ]; then
               # Using 32 physical cores per nodes
               export I_MPI_PIN_PROCESSOR_LIST="0-31"
           elif [ -n "$ALT1HT" ]; then
               export I_MPI_PIN_PROCESSOR_LIST="0-63"
           elif [ -n "$ALT2" ]; then
               # Using 16 physical cores per nodes
              export I_MPI_PIN_PROCESSOR_LIST="0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30"
           elif [ -n "$ALT2HT" ]; then
              export I_MPI_PIN_PROCESSOR_LIST="0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62"
           elif [ -n "$ALT4" ]; then
               # Using 8 physical cores per nodes
              export I_MPI_PIN_PROCESSOR_LIST="0,4,8,12,16,20,24,28"
           elif [ -n "$ALT4HT" ]; then
              export I_MPI_PIN_PROCESSOR_LIST="0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60"
           elif [ -n "$ALT8" ]; then
               # Using 4 physical cores per nodes
              export I_MPI_PIN_PROCESSOR_LIST="0,8,16,24"
           elif [ -n "$ALT8HT" ]; then
              export I_MPI_PIN_PROCESSOR_LIST="0,8,16,24,32,40,48,56"
           fi
    fi
    if [ -n "${AMD7502}" -o -n "${AMD7601}" -o -n "${AMD7532}" -o -n "${AMD7542}" ]; then
           if [ -z "$ALT1" -a -z "$ALT1HT" -a -z "$ALT2" -a -z "$ALT2HT" -a -z "$ALT4" -a -z "$ALT4HT" -a -z "$ALT8" -a -z "$ALT8HT" ]; then
              echo "Warning: An ALT pin value was not supplied or incorrect" >> $LOGFILE
              echo "Use: ALT1, ALT1HT, ALT2, ALT2HT, ALT4, ALT4HT, ALT8, ALT8HT" >> $LOGFILE
              echo "the integer in the ALTx is the number of cores to alternate between pinnings" >> $LOGFILE
              echo "Will use an ALT1 value to pin to cores 0 to 63 sequentially" >> $LOGFILE
              export ALT1=1
           fi
           if [ -n "$ALT1" ]; then
               export I_MPI_PIN_PROCESSOR_LIST="0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63"
           elif [ -n "$ALT1HT" ]; then
               export I_MPI_PIN_PROCESSOR_LIST="0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127"
           elif [ -n "$ALT2" ]; then
              # Using 32 core per node
              export I_MPI_PIN_PROCESSOR_LIST="0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62"
           elif [ -n "$ALT2HT" ]; then
              # Using 32 core per node
              export I_MPI_PIN_PROCESSOR_LIST="0,64,2,66,4,68,6,70,8,72,10,74,12,76,14,78,16,80,18,82,20,84,22,86,24,88,26,90,28,92,30,94,32,96,34,98,36,100,38,102,40,104,42,106,44,108,46,110,48,112,50,114,52,116,54,118,56,120,58,122,60,124,62,126"
           elif [ -n "$ALT4" ]; then
              # Using 16 core per node
              export I_MPI_PIN_PROCESSOR_LIST="0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60"
           elif [ -n "$ALT4HT" ]; then
              export I_MPI_PIN_PROCESSOR_LIST="0,64,4,68,8,72,12,76,16,80,20,84,24,88,28,92,32,96,36,100,40,104,44,108,48,112,52,116,56,120,60,124"
           elif [ -n "$ALT8" ]; then
              # Using 8 core per node
              export I_MPI_PIN_PROCESSOR_LIST="1,9,17,25,33,41,49,57,65,73,81,89,97,105,113,121"
           elif [ -n "$ALT8HT" ]; then
              # Using 8 core per node
              export I_MPI_PIN_PROCESSOR_LIST="0,64,8,72,16,80,24,88,32,96,40,104,48,112,56,120"
           fi
    elif [ -n "${AMD75F3_24c}" ]; then
           echo "DBG: Using AMD75F3_24c emulation"
           export I_MPI_PIN_PROCESSOR_LIST="1,2,3,4,6,7,8,9,12,13,14,15,17,18,19,20,23,24,25,26,28,29,30,31,33,34,35,36,38,39,40,41,44,45,46,47,48,49,50,51,53,54,55,56,59,60,61,63"
    elif [ -n "${AMD7702}" -o -n "${AMD7742}" ]; then
           if [ -z "$ALT1" -a -z "$ALT1HT" -a -z "$ALT2" -a -z "$ALT2HT" -a -z "$ALT4" -a -z "$ALT4HT" -a -z "$ALT8" -a -z "$ALT8HT" -a -z "$ALT16" -a -z "$ALT16HT" -a -z "$ALT16HT" -a -z "$ALT4MBcashPCore" -a -z "$ALT8MBcashPCore" -a -z "$ALT96" ]; then
              echo "Warning: An ALT pin value was not supplied or incorrect" >> $LOGFILE
              echo "Use: ALT1, ALT1HT, ALT2, ALT2HT, ALT4, ALT4HT, ALT8, ALT8HT, ALT16, ALT16HT" >> $LOGFILE
              echo "the integer in the ALTx is the number of cores to alternate between pinnings" >> $LOGFILE
              echo "Will use an ALT1 value to pin to cores 0 to 127 sequentially" >> $LOGFILE
              export ALT1=1
           fi
           if [ -n "$ALT1" ]; then
               export I_MPI_PIN_PROCESSOR_LIST="0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127"
           elif [ -n = "$ALT1HT" ]; then
	       seq_str=`seq 0 255`
               export I_MPI_PIN_PROCESSOR_LIST="`echo $seq_str | sed -e "s/\ /,/g"`"
           elif [ -n "$ALT2" ]; then
              export I_MPI_PIN_PROCESSOR_LIST="0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126"
           elif [ -n "$ALT2HT" ]; then
              export I_MPI_PIN_PROCESSOR_LIST="0,128,2,130,4,132,6,134,8,136,10,138,12,140,14,142,16,144,18,146,20,148,22,150,24,152,26,154,28,156,30,158,32,160,34,162,36,164,38,166,40,168,42,170,44,172,46,174,48,176,50,178,52,180,54,182,56,184,58,186,60,188,62,190,64,192,66,194,68,196,70,198,72,200,74,202,76,204,78,206,80,208,82,210,84,212,86,214,88,216,90,218,92,220,94,222,96,224,98,226,100,228,102,230,104,232,106,234,108,236,110,238,112,240,114,242,116,244,118,246,120,248,122,250,124,252,126,254"
           elif [ -n "$ALT4" ]; then
              export I_MPI_PIN_PROCESSOR_LIST="0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60,64,68,72,76,80,84,88,92,96,100,104,108,112,116,120,124"
           elif [ -n "$ALT4HT" ]; then
              export I_MPI_PIN_PROCESSOR_LIST="0,128,4,132,8,136,12,140,16,144,20,148,24,152,28,156,32,160,36,164,40,168,44,172,48,176,52,180,56,184,60,188,64,192,68,196,72,200,76,204,80,208,84,212,88,214,92,218,96,222,100,226,104,230,108,236,112,240,116,244,120,248,124,252"
           elif [ -n "$ALT8" ]; then
              export I_MPI_PIN_PROCESSOR_LIST="0,8,16,24,33,40,48,56,64,72,80,88,96,104,112,120"
           elif [ -n "$ALT8HT" ]; then
              export I_MPI_PIN_PROCESSOR_LIST="0,128,8,136,16,144,24,152,32,160,40,168,48,176,56,184,64,192,72,200,80,208,88,216,96,224,104,232,112,240,120,248"
           elif [ -n "$ALT16" ]; then
              export I_MPI_PIN_PROCESSOR_LIST="0,16,32,48,64,80,96,112"
           elif [ -n "$ALT16HT" ]; then
              export I_MPI_PIN_PROCESSOR_LIST="0,128,16,144,32,160,48,176,64,192,80,208,96,224,112,240"
           elif [ -n "$ALT4MBcashPCore" ]; then
             echo "Using ALT4MBcashPCore pinning for AMD7702/7742 ..." >> $LOGFILE
              export I_MPI_PIN_PROCESSOR_LIST="0,1,2,3,4,5,6,7,16,17,18,19,20,21,22,23,32,33,34,35,36,37,38,39,48,49,50,51,52,53,54,55,56,64,65,66,67,68,69,70,71,80,81,82,83,84,85,86,87,96,97,98,99,100,101,102,103,112,113,114,115,116,117,118,119"
           elif [ -n "$ALT8MBcashPCore" ]; then
             echo "Using ALT8MBcashPCore pinning for AMD7702/7742 ..." >> $LOGFILE
              export I_MPI_PIN_PROCESSOR_LIST="0,1,4,5,8,9,12,13,16,17,20,21,24,25,28,29,32,33,36,37,40,41,44,45,48,49,52,53,56,57,60,61,64,65,68,69,72,73,76,77,80,81,84,85,88,89,92,93,96,97,100,101,104,105,108,109,112,113,116,117,120,121,124,125"
           elif [ -n "$ALT96" ]; then
             export I_MPI_PIN_PROCESSOR_LIST="0-2,4-6,8-10,12-14,16-18,20-22,24-26,28-30,32-34,36-38,40-42,44-46,48-50,52-54,56-58,60-62,64-66,68-70,72-74,76-78,80-82,84-86,88-90,92-94,96-98,100-102,104-106,108-110,112-114,116-118,120-122,124-126"

           fi
    elif [ -z "${PPN}" -a -z "${USER_PPN}" ]; then
         export PPN=128
    elif [ "${PPN}" = "256" ]; then
       export I_MPI_PIN_PROCESSOR_LIST="0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255"
       seq_str=`seq 0 255`
       export I_MPI_PIN_PROCESSOR_LIST="`echo $seq_str | sed -e "s/\ /,/g"`"
    elif [ "${PPN}" = "128" ]; then
       export I_MPI_PIN_PROCESSOR_LIST="0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127"
       seq_str=`seq 0 127`
       export I_MPI_PIN_PROCESSOR_LIST="`echo $seq_str | sed -e "s/\ /,/g"`"
    elif [ "${PPN}" = "88" ]; then
       export I_MPI_PIN_PROCESSOR_LIST="0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87"
    elif [ "${PPN}" = "72" ]; then
       export I_MPI_PIN_PROCESSOR_LIST="0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71"
    elif [ "${PPN}" = "64" ]; then
       export I_MPI_PIN_PROCESSOR_LIST="0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63"
    elif [ "${PPN}" = "64_32S2" ]; then
       export I_MPI_PIN_PROCESSOR_LIST="0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62"
    elif [ "${PPN}" = "56" ]; then
       export I_MPI_PIN_PROCESSOR_LIST="0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55"
    elif [ "${PPN}" = "48" ]; then
       export I_MPI_PIN_PROCESSOR_LIST="0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47"
    elif [ "${PPN}" = "44" ]; then
       export I_MPI_PIN_PROCESSOR_LIST="0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43"
    elif [ "${PPN}" = "40" ]; then
       export I_MPI_PIN_PROCESSOR_LIST="0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39"
    elif [ "${PPN}" = "36" ]; then
       export I_MPI_PIN_PROCESSOR_LIST="0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35"
    elif [ "${PPN}" = "32" ]; then
       export I_MPI_PIN_PROCESSOR_LIST="0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31"
    elif [ "${PPN}" = "28" ]; then
       export I_MPI_PIN_PROCESSOR_LIST="0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27"
    elif [ "${PPN}" = "24" ]; then
       export I_MPI_PIN_PROCESSOR_LIST="0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23"
    elif [ "${PPN}" = "20" ]; then
       export I_MPI_PIN_PROCESSOR_LIST="0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19"
    elif [ "${PPN}" = "16" ]; then
       export I_MPI_PIN_PROCESSOR_LIST="0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15"
    elif [ "${PPN}" = "12" ]; then
       export I_MPI_PIN_PROCESSOR_LIST="0,1,2,3,4,5,6,7,8,9,10,11"
    elif [ "${PPN}" = "8" ]; then
       export I_MPI_PIN_PROCESSOR_LIST="0,1,2,3,4,5,6,7"
    elif [ "${PPN}" = "6" ]; then
       export I_MPI_PIN_PROCESSOR_LIST="0,1,2,3,4,5"
    elif [ "${PPN}" = "4" ]; then
       export I_MPI_PIN_PROCESSOR_LIST="0,1,2,3"
    elif [ -n "$USER_PPN" ]; then
       export I_MPI_PIN_PROCESSOR_LIST=$USER_PPN
       echo "Using USER_PPN defined value" >> $LOGFILE
       echo "USER_PPN=$USER_PPN" >> $LOGFILE
    else
       echo "Unrecognised PPN pin value, use 256, 128, 88, 72, 64, 56, 48, 44, 40, 36, 32, 28, 24, 20, 16, 12, 8, 6 or 4 " >> $LOGFILE
       echo "Alternatively, you may defne your own PPN comma seperated core list using USER_PPN environment variable" >> $LOGFILE
       exit
    fi
    echo "*** I_MPI_PIN_PROCESSOR_LIST=$I_MPI_PIN_PROCESSOR_LIST" >> $LOGFILE
fi

     if [ -n "$OPENSS_PROF" ]; then
        echo "*** Open Speedshop Profiling requested ..."
        echo "*** Loading Open Speedshop Profiler module ..."
        . /usr/share/Modules/init/bash
        module use /sw/com/modulefiles
        module load openspeedshop/openspeedshop-2.4.1
	export PATH=/sw/sdev/openspeedshop/x86_64/2.4.1/spack/opt/spack/linux-sles12-x86_64/gcc-4.8/python-2.7.16-qgvg37ofn6yfgqt56qpdcrnpe5a4gwqr/bin:/sw/sdev/openspeedshop/x86_64/2.4.1/spack/opt/spack/linux-sles12-x86_64/gcc-4.8/cbtf-krell-1.9.3-m7ypp7nz2vmeqx3afcgdpweutcln7wrk/sbin:/sw/sdev/openspeedshop/x86_64/2.4.1/spack/opt/spack/linux-sles12-x86_64/gcc-4.8/cbtf-krell-1.9.3-m7ypp7nz2vmeqx3afcgdpweutcln7wrk/bin:/sw/sdev/openspeedshop/x86_64/2.4.1/spack/opt/spack/linux-sles12-x86_64/gcc-4.8/mrnet-5.0.1-3-yvb2ezldngabqmv7qgae2zyrpfrgyb2z/bin:/sw/sdev/openspeedshop/x86_64/2.4.1/spack/opt/spack/linux-sles12-x86_64/gcc-4.8/openspeedshop-2.4.1-euyhl3iisikdhr44lkhaflbemi6gi5rv/bin:/home/users/ayad/.local/bin:/home/users/ayad/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/users/ayad/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/sbin:/bin:$PATH
      #export LD_LIBRARY_PATH=/sw/sdev/openspeedshop/x86_64/2.4.1/spack/opt/spack/linux-sles12-x86_64/gcc-4.8/openspeedshop-2.4.1-euyhl3iisikdhr44lkhaflbemi6gi5rv/lib64:$LD_LIBRRAY_PATH
        if [ -n "$OPENSS_TYPE" ]; then
             if [ $OPENSS_TYPE != osspcsamp ] && [ $OPENSS_TYPE != ossusertime ] && [ $OPENSS_TYPE != osshwc ] && [ $OPENSS_TYPE != osshwctime ] && [ $OPENSS_TYPE != ossio ] && [ $OPENSS_TYPE != ossiot ] && [ $OPENSS_TYPE != ossmpi ] && [ $OPENSS_TYPE != ossmpit ] && [ $OPENSS_TYPE != ossmpiotf ] && [ $OPENSS_TYPE != ossfpe ] ; then
                echo "*** Unkown OpenSpeedShop profiling type " >> ${LOGFILE} 2>&1
                echo "*** Allowed types for OPENSS_TYPE are: pcsamp usertime hwc hwctime io iot mpi mpit mpiot fpe " >> ${LOGFILE} 2>&1
                echo "*** Exiting ... " >> ${LOGFILE} 2>&1
             fi
        else
             export OPENSS_TYPE=osspcsamp
        fi
        export WHICH_PROF=$OPENSS_TYPE
        echo "***  OpenSpeedShop $OPENSS_TYPE Profiling Selected ..." >> ${LOGFILE} 2>&1
     fi

echo "Testing for MPITYPE: Compute sections" >> ${LOGFILE}

if [ "$WM_MPLIB" = "SGIMPI" -o "$WM_MPLIB" = "HMPTMPI" ]; then
     # WM_MPLIB has been set in setup.sh in Installation top directory
     #if [ "$WM_MPLIB" = "SGIMPI" ]; then
     #  export PATH="$HOME/mpt/$MPT_VERSION/bin:$PATH"
     #  export LD_LIBRARY_PATH="$HOME/mpt/$MPT_VERSION/lib:${LD_LIBRARY_PATH}"
     #fi
     #if [ "$WM_MPLIB" = "HMPTMPI" ]; then
     #  export PATH="$HOME/hmpt/$HMPT_VERSION/bin:$PATH"
     #  export LD_LIBRARY_PATH="$HOME/hmpt/$HMPT_VERSION/lib:${LD_LIBRARY_PATH}"
     #fi
     export MPI_VERBOSE=1
    #export MPI_DISPLAY_SETTINGS=1

     if [ -z "$SLURM_JOBID" ]; then
      if [ -f $PBS_NODEFILE -a -n "$PBS_VERSION" ]; then
          source /etc/pbs.conf
          attcmd=${PBS_EXEC}/bin/pbs_attach
          if [ -x $attcmd ] ; then
              export MPI_BATCH_CMD="$attcmd -j $PBS_JOBID"
          else
              export MPI_BATCH_CMD=''
              echo "mpiexec_mpt warning: $attcmd does not exist" >> $LOGFILE
          fi
      fi
     fi

     if [ -d "/proc/sgi_uv" ]; then
        # use the env var below for UV only
        echo "***  UV is assumed as the underlying compute hardware ..." >> ${LOGFILE} 2>&1
        export MPI_SHARED_NEIGHBORHOOD=HOST
        if [ -z "$CORES" ]; then
           export MPI_DSM_DISTRIBUTE=
        else
           export MPI_DSM_CPULIST="$CORES"
        fi
        export MPI_GRU_ENABLED=false
        mpirun -np $NPROC `which $EXE` \
               -case . -parallel < /dev/null >> ${LOGFILE} 2>&1
        exit
     fi
     if [ -z "$FABRIC" ]; then
          echo "Fabric not specified, MPT will use its default Fabric " >> ${LOGFILE}
     elif [ "$FABRIC" = "OPA" ]; then
          export MPI_USE_OPA=1
          export MPI_OPA_DEVS=1
        # export PSM2_MAX_CONTEXTS_PER_JOB=16
        # export PSM2_NUM_SEND_BUFFERS=4096
        # export PSM2_RANKS_PER_CONTEXT=2
     elif [ "$FABRIC" = "IB" ]; then
          export MPI_USE_IB=1
     elif [ "$FABRIC" = "TCP" ]; then
          export MPI_USE_TCP=1
     else
          echo "*** Unkown Fabric, exiting ... " >> ${LOGFILE}
          exit
     fi
     if [ -n "${DUAL_RAIL}" ]; then
      #  if [ -f /etc/mpt/multi-rail.cfg ]; then
      #     grep " 2 " /etc/mpt/multi-rail.cfg > tmp$$
      #     if [ $? = 0 ]; then
      #        rm tmp$$
      #        export MPI_IB_RAILS=2
      #     else
      #        echo Please Dual Rail request if enabled
      #        exit
      #     fi
      #  else
      #     echo /etc/mpt/multi-rail.cfg file does not exist
      #     echo Please Dual Rail request
      #     exit
      #  fi
        if [ "$FABRIC" = "OPA" ]; then
         export MPI_OPA_RAILS=2
        else
         export MPI_IB_RAILS=2
        fi
     else
        echo "Single IB Rail ..." >> ${LOGFILE}
     fi
     # mpirun_hosts_list:
     if [ -z "$BY_NODE" ]; then
        hosts_list=""
        mpirun_hosts_list=""
        host_token="0"
        host_np=$(expr 0)
        for i in `cat $PBS_NODEFILE`
        do
           hosts_list="$hosts_list $i"
        done
        for i in $hosts_list
        do
           if [ "$host_token" = "0" ]; then
             # this is the first name in the list of hosts
             host_token="$i"
           fi
           if [ "$host_token" = "$i" ]; then
             # this is a repeated host list in the sequence
             # thus increment the associated count host_np
             host_np=`expr $host_np + 1`
           else
             # a new hostname has now appeared in the list
             # thus update the hosts_list with the host_token
             # and its associated process count
             # and set host_token to the new hostname
             # and reset the new host_token associated process count
             # to 1
             mpirun_hosts_list="$mpirun_hosts_list,$host_token $host_np"
             host_token="$i"
             host_np=$(expr 1)
           fi
        done
        # update mpirun_hosts_list with the last entry
        mpirun_hosts_list="$mpirun_hosts_list,$host_token $host_np"
        # remove the leading coma in mpirun_hosts_list
        mpirun_hosts_list=`echo $mpirun_hosts_list| sed -e "s/,//"`
     else
         entries=`wc -l $PBS_NODEFILE| awk '{print $1}'`
         echo DBG: entries=$entries > tmp.out
         cat $PBS_NODEFILE| uniq > BLK_FILE
         blk_len=$NUM_NODES
         num_blks=`expr $entries / $blk_len`
         echo DBG blk_len=$blk_len num_blks=$num_blks >> tmp.out
         rm -f BY_NODE_NODEFILE 
         touch BY_NODE_NODEFILE
         for j in `seq 1  $num_blks`
         do
             cat BLK_FILE >> BY_NODE_NODEFILE
         done
         export PBS_NODEFILE=BY_NODE_NODEFILE
     fi
     echo "DBG `wc -l $PBS_NODEFILE`" >> tmp.out
     #
     # Profiling sections: MPInside, Perfcatcher and Psrun
     #
     export WHICH_PROF=""
     #
     if [ -n "${MPINSIDE_PROF}" ]; then
        echo "***  Performing MPIndside profiling ..." >> ${LOGFILE} 2>&1
        . /usr/share/Modules/init/bash
        module use /sw/com/modulefiles
       #module load MPInside/3.6.4-beta5
       #module load MPInside/3.6.7
       #module load MPInside/4.2.48
       #module load MPInside/tot
        module load MPInside/3.6.1
        export WHICH_PROF="`which MPInside`"
        rm -rf MPINSIDE_MAT_DIR
        cp $PBS_NODEFILE mpiplace.file
ed mpiplace.file << MYEOF
2,\$s/^/ /
1,\$j
w
q
MYEOF
        export MPINSIDE_SHOW_READ_WRITE=1
        export MPINSIDE_PRINT_ALL_COLUMNS=1
        export MPINSIDE_PRINT_DIRTY=1
        export MPINSIDE_VERBOSE=1
        export MPINSIDE_OUTPUT_PREFIX=mpinside_${MPINSIDE_PROF}_$CLK_SPD
        export MPINSIDE_DEBUG_FILE_PREFIX=$PBS_O_WORKDIR/debug_mpinside_rank_
        export MPINSIDE_BINARY_MATRICES_DIR=MPINSIDE_MAT_DIR/
      #
        if [ "$MPINSIDE_PROF" = "basic" ]; then
          #export MPINSIDE_SIZE_DISTRIBUTION="24:0-63"
           export MPINSIDE_ENABLE_COLLECTIVE_WAIT_TIME=true
        elif [ "$MPINSIDE_PROF" = "lite" ]; then
           export MPINSIDE_LITE=1
        elif [ "$MPINSIDE_PROF" = "slt" ]; then
           export MPINSIDE_EVAL_COLLECTIVE_WAIT=1
           export MPINSIDE_EVAL_SLT=1
        elif [ "$MPINSIDE_PROF" = "slt_b" ]; then
           export MPINSIDE_EVAL_COLLECTIVE_WAIT=1
           export MPINSIDE_EVAL_SLT_B=1
           export MPINSIDE_OUTPUT_PREFIX=mpinside_slt_B
           export MPINSIDE_MULT_MODEL=-1.0
        elif [ "$MPINSIDE_PROF" = "perfect" ]; then
           export MPINSIDE_MODEL=PERFECT+1.0
           export MPINSIDE_MATRICES=EXA:-B:S
        elif [ "$MPINSIDE_PROF" = "perfect_b" ]; then
           export MPINSIDE_MODEL=PERFECT_B+1.0
           export MPINSIDE_MULT_MODEL=1.0
           export MPINSIDE_TRF_DEF_0=12@0:0@0+0:9999999999999@0+0:9999999999999
        elif [ "$MPINSIDE_PROF" = "collectivewait" ]; then
           export MPINSIDE_EVAL_COLLECTIVE_WAIT=1
        else
           echo "*** Unkown MPInside profile selector, please use: " >> ${LOGFILE} 2>&1
           echo "*** Unkown MPInside profile selector, please use: "
           echo "*** basic, slt, slt_b, perfect, perfect_b or collectivewait" >> ${LOGFILE} 2>&1
           echo "*** basic, slt, slt_b, perfect, perfect_b or collectivewait"
           echo "*** Existing ... " >> ${LOGFILE} 2>&1
           echo "*** Existing ... "
           exit
        fi
     fi
     if [ -n "${MPIPLACE_TASK}" ]; then
        echo "***  Performing mpiplace ... " >> ${LOGFILE} 2>&1
        echo "***  MPIPLACE_TASK=$MPIPLACE_TASK" >> ${LOGFILE} 2>&1
        . /usr/share/Modules/init/bash
        module use /sw/com/modulefiles
        module load mpiplace
        if [ ! -f mpiplace.file ]; then
           echo "***  mpiplace.file does not exist" >> ${LOGFILE} 2>&1
           echo "***  exiting ..." >> ${LOGFILE} 2>&1
           exit
        fi
        if [ ! -d MPINSIDE_MAT_DIR ]; then
           echo "***  MPINSIDE_MAT_DIR does not exist" >> ${LOGFILE} 2>&1
           echo "***  exiting ..." >> ${LOGFILE} 2>&1
           exit
        fi
     #  export LD_PRELOAD=$FOAM_EXT_LIBBIN/2.07-ga/libscotch.so
        mpirun -v -np 1 `which mpiplace` -N `cat mpiplace.file` MPINSIDE_MAT_DIR >> ${LOGFILE} 2>&1
        if [ ! -f mpiplace_perm ]; then
           echo "***  mpiplace_perm Permutation file was not generated!" >> ${LOGFILE} 2>&1
           echo "***  exiting ..." >> ${LOGFILE} 2>&1
           exit
        fi
        export MPI_WORLD_MAP=mpiplace_perm
     fi
     if [ -n "$OPENSS_PROF" ]; then
        echo "*** Open Speedshop Profiling requested ..."
        echo "*** Loading Open Speedshop Profiler module ..."
        . /usr/share/Modules/init/bash
        module use /sw/com/modulefiles
        module load openspeedshop/openspeedshop-2.4.1
        if [ -n "$OPENSS_TYPE" ]; then
             if [ $OPENSS_TYPE != osspcsamp ] && [ $OPENSS_TYPE != ossusertime ] && [ $OPENSS_TYPE != osshwc ] && [ $OPENSS_TYPE != osshwctime ] && [ $OPENSS_TYPE != ossio ] && [ $OPENSS_TYPE != ossiot ] && [ $OPENSS_TYPE != ossmpi ] && [ $OPENSS_TYPE != ossmpit ] && [ $OPENSS_TYPE != ossmpiotf ] && [ $OPENSS_TYPE != ossfpe ] ; then
                echo "*** Unkown OpenSpeedShop profiling type " >> ${LOGFILE} 2>&1
                echo "*** Allowed types for OPENSS_TYPE are: pcsamp usertime hwc hwctime io iot mpi mpit mpiot fpe " >> ${LOGFILE} 2>&1
                echo "*** Exiting ... " >> ${LOGFILE} 2>&1
             fi
        else
             export OPENSS_TYPE=osspcsamp
        fi
        export WHICH_PROF=$OPENSS_TYPE     
        echo "***  OpenSpeedShop $OPENSS_TYPE Profiling Selected ..." >> ${LOGFILE} 2>&1
     fi
     if [ -n "$PERFCATCHER_PROF" ]; then
      # export PATH=/opt/sgi/perfcatcher/bin:$PATH
      # export LD_LIBRARY_PATH=/opt/sgi/perfcatcher/lib:${LD_LIBRARY_PATH} 
        module use /usr/share/Modules/modulefiles
        module load perfcatcher
        export WHICH_PROF="`which perfcatch`"
        echo "***  Perfcatcher Profiling Selected ... $WHICH_PROF " >> ${LOGFILE} 2>&1
     fi
     if [ -n "$PSRUN_PROF" ]; then
        . /usr/share/Modules/init/bash
        module use /sw/com/modulefiles
        module load perfsuite/1a5.3
        export WHICH_PROF="`which psrun` -f "
        echo "***  psrun Profiling Selected ..." >> ${LOGFILE} 2>&1
     fi
     #export MPI_IB_DEVS=1
     #export MPI_COLL_OPT=false
    #export MPI_BUFS_LIMIT=128
    # export MPI_IB_RECV_BUFS=512
     #export MPI_HUGEPAGE_MSGS=false
    #export MPI_BUFS_PER_PROC=4096
    #export MPI_IB_SINGLE_COPY_BUFFER_MAX=66384
    #export MPI_HUGEPAGE_MSGS=1
     if [ -n "$IB_DEV" ]; then
           export MPI_IB_DEVS=$IB_DEV
     fi
     export MPI_VERBOSE=1
     # If using /dev/shm or local disks
     # Test first if USE_DEVSHM  or USE_LDISK is set
     if [ -n "$USE_DEVSHM" -o -n "$USE_LDISK" ]; then
             #
              if [ -n "$USE_DEVSHM" ]; then
                   export LOCAL_DISK_PATH=/dev/shm
              fi
              if [ -n "$USE_LDISK" ]; then
                   export LOCAL_DISK_PATH=$USE_LDISK
              fi
              echo "DBG: LOCAL_DISK_PATH=$LOCAL_DISK_PATH"  >> ${LOGFILE} 2>&1
              cd $SCRATCH_DIR
              echo "*** $SCRATCH_DIR is now working directory ..."  >> ${LOGFILE} 2>&1
              echo "*** Making tarball ${NPROC}_d_local.tar ..." >> ${LOGFILE} 2>&1
              tar cf ${NPROC}_d_local.tar ${NPROC}_d
              echo "`ls -l ${NPROC}_d_local.tar`"  >> ${LOGFILE} 2>&1
              hosts_list=""
              for i in `cat $PBS_NODEFILE | sort -u `
              do
                 hosts_list="$hosts_list $i"
              done
              c=$(expr 0)
              for i in $hosts_list
              do
                 c=`expr $c + 1`
                 echo "*** c = $c ... " >> ${LOGFILE} 2>&1
                 echo "Copying tarball to $i and untaring" >> ${LOGFILE} 2>&1
                 ssh $i cp $PBS_O_WORKDIR/${NPROC}_d_local.tar $LOCAL_DISK_PATH
                 if [ "$c" = "$NUM_NODES" ]; then
                    echo "*** last node untar ..." >> ${LOGFILE} 2>&1
                    ssh $i "cd $LOCAL_DISK_PATH ; tar xf ${NPROC}_d_local.tar "
                 else
                    ssh -n -f $i "cd $LOCAL_DISK_PATH ; nohup tar xf ${NPROC}_d_local.tar ; rm ${NPROC}_d_local.tar"
                 fi
                 ssh $i ls -l $LOCAL_DISK_PATH/${NPROC}_d_local.tar >> ${LOGFILE} 2>&1
              done
              echo "Sending tarballs done ..." >> ${LOGFILE} 2>&1
              rm ${NPROC}_d_local.tar
              cd $LOCAL_DISK_PATH/${NPROC}_d
              echo "*** Changing to `pwd` " >> ${LOGFILE} 2>&1
              export MPI_DIR=$LOCAL_DISK_PATH/${NPROC}_d
              # Add OPENMPI mpirun set commands
              #
              echo "mpirun /${NPROC}_d $mpirun_hosts_list $WHICH_PROF `which $EXE` -case $LOCAL_DISK_PATH/${NPROC}_d -parallel " >> ${LOGFILE} 2>&1
              mpirun -d $LOCAL_DISK_PATH/${NPROC}_d $mpirun_hosts_list $WHICH_PROF `which $EXE` -parallel < /dev/null >> ${LOGFILE} 2>&1
              if [ -n "$CP_DEVSHM_RES" ]; then
                 cd $SCRATCH_DIR
                 echo "*** Copying results from $LOCAL_DISK_PATH ... " >> ${LOGFILE} 2>&1
                 for i in $hosts_list
                 do
                     echo "*** Copying from $i to `pwd` ... " >> ${LOGFILE} 2>&1
                     ssh $i "cd $LOCAL_DISK_PATH ; tar cf ${NPROC}_d_back_${i}.tar ${NPROC}_d"
                     ssh $i cp $LOCAL_DISK_PATH/${NPROC}_d_back_${i}.tar $SCRATCH_DIR
                     tar xf ${NPROC}_d_back_${i}.tar
                     rm ${NPROC}_d_back_${i}.tar
                     ssh $i rm -rf $LOCAL_DISK_PATH/${NPROC}_d
                     ssh $i rm -rf $LOCAL_DISK_PATH/$LOCAL_DISK_PATH/${NPROC}_d_back_${i}.tar
                 done
                 echo "*** Copying results complete" >> ${LOGFILE} 2>&1
              else
                     ssh $i rm -rf $LOCAL_DISK_PATH/${NPROC}_d
              fi
     else
           #export MPI_SYNC_THRESHOLD=16384
           #- For parallel running
           if [ -n "$CRAY" ]; then
             #export CRAY_OPTION="mpi=cray_shasta"
           ###export MPI_WHICH_PMI=cray_shasta
           ###srun  --cpu-bind=none
             #export CRAY_OPTION="--mpi=list"
	     echo "Testing for keywork CRAY"
           else
              export CRAY_OPTION=""
           fi
           if [ -z "$OPENSS_PROF" ]; then
             #echo "DBG: GCC Compiler `which g++`"
              echo "*** mpirun is `which mpirun` " >> ${LOGFILE}
              echo "DBG: `which mpiexec_mpt` " >> ${LOGFILE}
              echo "mpiexec_mpt $WHICH_PROF `which $EXE`  $AFIN -parallel " >> ${LOGFILE}
              echo "running $EXE"  >> $LOGFILE
              echo "PBS_NODEFILE=$PBS_NODEFILE" >> $LOGFILE
            # export MPI_USING_VTUNE=true
              echo "time mpiexec_mpt $CRAY_OPTION $WHICH_PROF $AFIN `which $EXE` $FOAM_OPTIONS -parallel" >> ${LOGFILE}
              (time mpiexec_mpt $CRAY_OPTION $WHICH_PROF $AFIN `which $EXE` $FOAM_OPTIONS -parallel \
                      < /dev/null) >> ${LOGFILE} 2>&1
           else
     #        export MPI_SHEPHERD=true
              printenv
             #echo DBG: GCC Compiler `which g++` >> ${LOGFILE} 2>&1
              echo "$WHICH_PROF mpiexec_mpt $CRAY_OPTION `which $EXE` $FOAM_OPTIONS -parallel " >> ${LOGFILE} 2>&1
              $WHICH_PROF "`which mpiexec_mpt` $CRAY_OPTION $AFIN `which $EXE` -parallel " < /dev/null >> ${LOGFILE} 2>&1
           fi
     fi
#
elif [ "$WM_MPLIB" = "OPENMPI" -o "$WM_MPLIB" = "SYSTEMOPENMPI" ]; then
     #
     #
     export WHICH_PROF=""
     #
     if [ -n "${MPINSIDE_PROF}" ]; then
        echo "***  Performing MPIndside profiling ..." >> ${LOGFILE} 2>&1
        . /usr/share/Modules/init/bash
        module use /sw/com/modulefiles
       #module load MPInside/3.6.4-beta5
        module load MPInside/3.6.1
        export MPINSIDE_LIB=OPENMPI
        export WHICH_PROF="`which MPInside`"
        rm -rf MPINSIDE_MAT_DIR
        cp $PBS_NODEFILE mpiplace.file
ed mpiplace.file << MYEOF
2,\$s/^/ /
1,\$j
w
q
MYEOF
        export MPINSIDE_SHOW_READ_WRITE=1
        export MPINSIDE_PRINT_ALL_COLUMNS=1
        export MPINSIDE_PRINT_DIRTY=1
        export MPINSIDE_VERBOSE=1
        export MPINSIDE_OUTPUT_PREFIX=mpinside_${MPINSIDE_PROF}_$CLK_SPD
        export MPINSIDE_DEBUG_FILE_PREFIX=$PBS_O_WORKDIR/debug_mpinside_rank_
        export MPINSIDE_BINARY_MATRICES_DIR=MPINSIDE_MAT_DIR/
      #
        if [ "$MPINSIDE_PROF" = "basic" ]; then
           export MPINSIDE_SIZE_DISTRI=T+12:0-27 #[T+]nb_bars[:first-last]
        elif [ "$MPINSIDE_PROF" = "lite" ]; then
           export MPINSIDE_LITE=1
        elif [ "$MPINSIDE_PROF" = "slt" ]; then
           export MPINSIDE_EVAL_COLLECTIVE_WAIT=1
           export MPINSIDE_EVAL_SLT=1
        elif [ "$MPINSIDE_PROF" = "slt_b" ]; then
           export MPINSIDE_EVAL_COLLECTIVE_WAIT=1
           export MPINSIDE_EVAL_SLT_B=1
           export MPINSIDE_OUTPUT_PREFIX=mpinside_slt_B
           export MPINSIDE_MULT_MODEL=-1.0
        elif [ "$MPINSIDE_PROF" = "perfect" ]; then
           export MPINSIDE_MODEL=PERFECT+1.0
           export MPINSIDE_MATRICES=EXA:-B:S
        elif [ "$MPINSIDE_PROF" = "perfect_b" ]; then
           export MPINSIDE_MODEL=PERFECT_B+1.0
           export MPINSIDE_MULT_MODEL=1.0
           export MPINSIDE_TRF_DEF_0=12@0:0@0+0:9999999999999@0+0:9999999999999
        elif [ "$MPINSIDE_PROF" = "collectivewait" ]; then
           export MPINSIDE_EVAL_COLLECTIVE_WAIT=1
        else
           echo "*** Unkown MPInside profile selector, please use: " >> ${LOGFILE} 2>&1
           echo "*** Unkown MPInside profile selector, please use: "
           echo "*** basic, slt, slt_b, perfect, perfect_b or collectivewait" >> ${LOGFILE} 2>&1
           echo "*** basic, slt, slt_b, perfect, perfect_b or collectivewait"
           echo "*** Existing ... " >> ${LOGFILE} 2>&1
           echo "*** Existing ... "
           exit
        fi
     fi # End of MPIINSIDE section
     #
     # If using /dev/shm or local disks
     # Test first if USE_DEVSHM  or USE_LDISK is set
     if [ -n "$USE_DEVSHM" -o -n "$USE_LDISK" ]; then
             #
              if [ -n "$USE_DEVSHM" ]; then
                   export LOCAL_DISK_PATH=/dev/shm
              fi
              if [ -n "$USE_LDISK" ]; then
                   export LOCAL_DISK_PATH=$USE_LDISK
              fi
              echo "DBG: LOCAL_DISK_PATH=$LOCAL_DISK_PATH"  >> ${LOGFILE} 2>&1
              cd $SCRATCH_DIR
              echo "*** $SCRATCH_DIR is now working directory ..."  >> ${LOGFILE} 2>&1
              echo "*** Making tarball ${NPROC}_d_local.tar ..." >> ${LOGFILE} 2>&1
              tar cf ${NPROC}_d_local.tar ${NPROC}_d
              echo "`ls -l ${NPROC}_d_local.tar`"  >> ${LOGFILE} 2>&1
              hosts_list=""
              for i in `cat $PBS_NODEFILE | sort -u `
              do
                 hosts_list="$hosts_list $i"
              done
              c=$(expr 0)
              for i in $hosts_list
              do
                 c=`expr $c + 1`
                 echo "*** c = $c ... " >> ${LOGFILE} 2>&1
                 echo "Copying tarball to $i and untaring" >> ${LOGFILE} 2>&1
                 ssh $i cp $PBS_O_WORKDIR/${NPROC}_d_local.tar $LOCAL_DISK_PATH
                 if [ "$c" = "$NUM_NODES" ]; then
                    echo "*** last node untar ..." >> ${LOGFILE} 2>&1
                    ssh $i "cd $LOCAL_DISK_PATH ; tar xf ${NPROC}_d_local.tar "
                 else
                    ssh -n -f $i "cd $LOCAL_DISK_PATH ; nohup tar xf ${NPROC}_d_local.tar ; rm ${NPROC}_d_local.tar"
                 fi
                 ssh $i ls -l $LOCAL_DISK_PATH/${NPROC}_d_local.tar >> ${LOGFILE} 2>&1
              done
              echo "Sending tarballs done ..." >> ${LOGFILE} 2>&1
              rm ${NPROC}_d_local.tar
              cd $LOCAL_DISK_PATH/${NPROC}_d
              echo "*** Changing to `pwd` " >> ${LOGFILE} 2>&1
              export MPI_DIR=$LOCAL_DISK_PATH/${NPROC}_d
              # OPENMPI mpirun command
              echo "Running with OPENMPI ... " >> ${LOGFILE}
#             cat $PBS_NODEFILE |uniq > hostfile
#d hostfile << EOF
#,\$s/com/com cpu=$PPN/
#
#
#EOF
#
     cat hostfile

     export EXE_PATH=`which $EXE`
     cp $EXE_PATH ${SCRATCH_DIR}/${NPROC}_d
     export OPEN_MPI_PATH=$MPI_ARCH_PATH
     export MPI_ROOT=$MPI_ARCH_PATH
     export OPEN_MPILIB_PATH=$OPEN_MPI_PATH/lib
     export OPEN_MPIBIN_PATH=$OPEN_MPI_PATH/bin
     export OPAL_PREFIX=$OPEN_MPI_PATH
     export PMIX_INSTALL_PREFIX=$OPAL_PREFIX
     export OMPI_MCA_opal_common_ucx_opal_mem_hooks=1
     export OMPI_MCA_pml_ucx_verbose=100

      RUNFLAGS="--mca btl self,openib \
                --mca btl_openib_allow_ib 1 \
                --mca mpi_warn_on_fork 0 \
                --mca btl_openib_max_send_size 306144"

      RUNFLAGS="-x FOAM_SETTINGS \
                -x WM_PROJECT_INST_DIR=$WM_PROJECT_INST_DIR \
                -x WM_PROJECT_DIR=$WM_PROJECT_DIR           \
                -x PATH -x LD_LIBRARY_PATH \
                -x EXE_PATH \
                -x OPEN_MPI_PATH=$OPEN_MPI_PATH \
                -x OPEN_MPILIB_PATH=$OPEN_MPILIB_PATH \
                -x PBS_O_WORKDIR \
                -x MPI_ARCH_PATH \
                -x OPAL_PREFIX \
                -x PMIX_INSTALL_PREFIX \
                -x LIBRARY_PATH \
                -x OPEN_MPILIB_PATH \
                -x OPEN_MPIBIN_PATH \
                -x UCX_NET_DEVICES=mlx5_0:1 \
                -x HCOLL_MAIN_IB=mlx5_0:1 \
                -x OMPI_MCA_opal_common_ucx_opal_mem_hooks \
                -x NPROC \
                -x BIND_TO \
                -x DPLACE_PIN \
                -x DPLACE_BIND_TO \
                -x UCX_WARN_UNUSED_ENV_VARS=n \
                -x OMPI_MCA_pml_ucx_verbose $RUNFLAGS"
     echo "DBG:" >> $LOGFILE
     echo "DBG:*********** Local Disk Full Command Line **********" >> $LOGFILE
     echo "DBG:" >> $LOGFILE
     echo "`which mpirun` --prefix $OPEN_MPI_PATH -wdir $LOCAL_DISK_PATH/${NPROC}_d $RUNFLAGS -np ${NPROC} $BIND_TO -machinefile hostfile `which foamExec` ${DPLACE_PIN} $EXE_PATH -parallel -case  ${SCRATCH_DIR}/${NPROC}_d" >> $LOGFILE
     echo "DBG:" >> $LOGFILE

     `which mpirun` \
                 --prefix $OPEN_MPI_PATH -wdir $LOCAL_DISK_PATH/${NPROC}_d $RUNFLAGS \
                  -np ${NPROC} $BIND_TO -machinefile hostfile \
                 `which foamExec` ${DPLACE_PIN} $EXE_PATH -parallel \
                 -case  ${SCRATCH_DIR}/${NPROC}_d < /dev/null >> $LOGFILE 2>>1

              if [ -n "$CP_DEVSHM_RES" ]; then
                 cd $SCRATCH_DIR
                 echo "*** Copying results from $LOCAL_DISK_PATH ... " >> ${LOGFILE} 2>&1
                 for i in $hosts_list
                 do
                     echo "*** Copying from $i to `pwd` ... " >> ${LOGFILE} 2>&1
                     ssh $i "cd $LOCAL_DISK_PATH ; tar cf ${NPROC}_d_back_${i}.tar ${NPROC}_d"
                     ssh $i cp $LOCAL_DISK_PATH/${NPROC}_d_back_${i}.tar $SCRATCH_DIR
                     tar xf ${NPROC}_d_back_${i}.tar
                     rm ${NPROC}_d_back_${i}.tar
                     ssh $i rm -rf $LOCAL_DISK_PATH/${NPROC}_d
                     ssh $i rm -rf $LOCAL_DISK_PATH/$LOCAL_DISK_PATH/${NPROC}_d_back_${i}.tar
                 done
                 echo "*** Copying results complete" >> ${LOGFILE} 2>&1
              else
                     ssh $i rm -rf $LOCAL_DISK_PATH/${NPROC}_d
              fi
     else

        echo "Running with OPENMPI ... " >> ${LOGFILE}

        export PATH=$WM_THIRD_PARTY_DIR/UCX/$UCX_RUNTIME_VERSION/bin:$PATH
        export LD_LIBRARY_PATH=$WM_THIRD_PARTY_DIR/UCX/$UCX_RUNTIME_VERSION/lib:$WM_THIRD_PARTY_DIR/UCX/$UCX_RUNTIME_VERSION/lib/ucx:$LD_LIBRARY_PATH

        echo "*************************************"  >> ${LOGFILE} 2>&1
        echo "DBG: LD_LIBRARY_PATH=$LD_LIBRARY_PATH"  >> ${LOGFILE} 2>&1
        echo "*************************************"  >> ${LOGFILE} 2>&1
        echo "*************************************"  >> ${LOGFILE} 2>&1
        echo "DBG: MPI_ARCH_PATH=$MPI_ARCH_PATH"  >> ${LOGFILE} 2>&1
        echo "DBG: PWD=`pwd`" >> ${LOGFILE} 2>&1
        echo "*************************************"  >> ${LOGFILE} 2>&1


        cd ${SCRATCH_DIR}/${NPROC}_d
        export EXE_PATH=`which $EXE`
        cp $EXE_PATH ${SCRATCH_DIR}/${NPROC}_d
        export OPEN_MPI_PATH=$MPI_ARCH_PATH
        export MPI_ROOT=$MPI_ARCH_PATH
        export OPEN_MPILIB_PATH=$OPEN_MPI_PATH/lib
        export OPEN_MPIBIN_PATH=$OPEN_MPI_PATH/bin
        export OPAL_PREFIX=$OPEN_MPI_PATH
        export PMIX_INSTALL_PREFIX=$OPAL_PREFIX
#       export OMPI_MCA_opal_common_ucx_opal_mem_hooks=1
       #export OMPI_MCA_pml_ucx_verbose=100
    #   if [ -z "${OPENMPI_USE_RANKFILE}" ]; then

    #        cat $PBS_NODEFILE |uniq > hostfile
    #        cp hostfile ${SCRATCH_DIR}/${NPROC}_d
    #
    #
    #        RUNFLAGS="--mca btl self,openib \
    #                  --mca btl_openib_allow_ib 1 \
    #                  --mca btl_openib_max_send_size 306144 \
    #                  --mca mpi_warn_on_fork 0 "
    #   
    #        RUNFLAGS="-x FOAM_SETTINGS \
    #                  -x WM_PROJECT_INST_DIR=$WM_PROJECT_INST_DIR \
    #                  -x WM_PROJECT_DIR=$WM_PROJECT_DIR           \
    #     	       -x UCX_RUNTIME_VERSION \
    #                  -x PATH -x LD_LIBRARY_PATH -x OPEN_MPI_PATH=$OPEN_MPI_PATH \
    #                  -x PBS_O_WORKDIR \
    #                  -x MPI_ROOT \
    #                  -x OPAL_PREFIX \
    #                  -x PMIX_INSTALL_PREFIX \
    #                  -x LIBRARY_PATH \
    #                  -x OPEN_MPILIB_PATH \
    #                  -x OPEN_MPIBIN_PATH \
    #                  -x UCX_NET_DEVICES=mlx5_0:1 \
    #                  -x HCOLL_MAIN_IB=mlx5_0:1 \
    #                  -x OMPI_MCA_opal_common_ucx_opal_mem_hooks \
    #                  -x OMPI_MCA_pml_ucx_verbose \
    #                  -x BIND_TO \
    #                  -x DPLACE_PIN \
    #                  -x UCX_WARN_UNUSED_ENV_VARS=n \
    #                  $RUNFLAGS"
    #   
    #        echo "*************************************"  >> ${LOGFILE} 2>&1
    #        echo "DBG: Invoking OPENMPI mpirun"  >> ${LOGFILE} 2>&1
    #        echo "*************************************"  >> ${LOGFILE} 2>&1
    #        echo "DBG:" >> $LOGFILE
    #        echo "DBG:*********** Full Command Line **********" >> $LOGFILE
    #        echo "DBG:" >> $LOGFILE
    #        echo "`which mpirun` -wdir ${SCRATCH_DIR}/${NPROC}_d --prefix $OPEN_MPI_PATH $RUNFLAGS -np ${NPROC} $BIND_TO -machinefile hostfile ${DPLACE_PIN} $EXE_PATH -parallel -case  ${SCRATCH_DIR}/${NPROC}_d" >> $LOGFILE
    #        echo "DBG:" >> $LOGFILE
    #        `which mpirun` \
    #                     -wdir ${SCRATCH_DIR}/${NPROC}_d --prefix $OPEN_MPI_PATH $RUNFLAGS \
    #                     -np ${NPROC} $BIND_TO -machinefile hostfile \
    #                     ${DPLACE_PIN} $EXE_PATH -parallel \
    #                    -case  ${SCRATCH_DIR}/${NPROC}_d < /dev/null >> $LOGFILE 2>>1
    #   else
           # Using Rank file method
	   echo "*** Using Rank File Method for Pinning" >> $LOGFILE

	   export MACHINE_FILE=${PBS_NODEFILE}
	   echo "*** Listing machinefile" >> $LOGFILE
           cat ${MACHINE_FILE}
	   echo "*** end of Listing machinefile" >> $LOGFILE
           rm -rf ${SCRATCH_DIR}/${NPROC}_d/rk.file
	   ntasks=`cat $MACHINE_FILE | wc -l`
           echo "*** Number of lines in MACHINE_FILE = $ntasks " >> ${LOGFILE}
           if [ -n "$RANKFILE_USE_OLD_CODE" -o -n "$USER_PPN" ]; then
           #
           # Old code
           #
            rank=0
            echo "*** CORES_LIST=$CORES_LIST MACHINE_FILE=$MACHINE_FILE" >> $LOGFILE
            for node_name in `uniq ${MACHINE_FILE}`
            do
             for i in `echo $CORES_LIST`
             do
                    if [ $rank -lt $ntasks ]; then
                     echo "rank ${rank}=${node_name} slot=${i}" >> ${SCRATCH_DIR}/${NPROC}_d/rk.file
                     ((rank=rank+1))
                    fi
             done
            done
           else
          # New rank file method code
             hosts_list=""
             host_token="0"
             slot=$(expr 0)
             rank=$(expr 0)
             for i in `cat ${MACHINE_FILE}`
             do
                hosts_list="$hosts_list $i"
             done
             for i in $hosts_list
             do
     
                if [ "$host_token" = "0" ]; then
                  # this is the first name in the list of hosts
                  host_token="$i"
                  rank=$(expr 0)
                  slot=$(expr 0)
     
                fi
                if [ "$host_token" = "$i" ]; then
                  # this is a repeated host list in the sequence
                  #
     
                  echo "rank ${rank}=${host_token} slot=${slot}" >> ${SCRATCH_DIR}/${NPROC}_d/rk.file
     
                else
                  # a new hostname has now appeared in the list
                  # thus reset slot to zero and update the hosts_list with the new host_token
                  #
                  slot=$(expr 0)
     
                  host_token="$i"
                  echo "rank ${rank}=${host_token} slot=${slot}" >> ${SCRATCH_DIR}/${NPROC}_d/rk.file
     
                fi
                if [ "$ALT" = 1 ]; then
                      ((slot=slot+1))
                elif [ "$ALT" = "2" ]; then
                      ((slot=slot+2))
                elif [ "$ALT" = "4" ]; then
                      ((slot=slot+4))
                elif [ "$ALT" = "8" ]; then
                      ((slot=slot+8))
                elif [ "$ALT" = "16" ]; then
                      ((slot=slot+16))
                elif [ "$ALT" = "32" ]; then
                      ((slot=slot+32))
                else
                      ((slot=slot+1))
                fi
                ((rank=rank+1))
             done
     #     End of new rkfile generation code
           fi
           export MPI_BUFFER_SIZE=500000000
           export OMPI_MCA_rmaps_rank_file_physical=1
           echo "mpiopts=$mpiopts" >> ${LOGFILE} 2>&1
           export DPLACE_PIN="--bind-to hwthread --rankfile ${SCRATCH_DIR}/${NPROC}_d/rk.file"
           export RANK_FILE=${SCRATCH_DIR}/${NPROC}_d/rk.file
           echo "*** Number of lines in the rank file ="`wc -l $RANK_FILE `"" >> ${LOGFILE}
           echo "BIND_TO=$BIND_TO    DPLACE_PIN=$DPLACE_PIN" >> ${LOGFILE}
           export UCX_TLS="sm,rc_x,mm"
           mpiopts="$mpiopts \
                      -x FOAM_SETTINGS -x WM_PROJECT_DIR -x MPI_BUFFER_SIZE \
                      -x OPEN_MPI_PATH -x MPI_ROOT -x OPAL_PREFIX -x OPEN_MPILIB_PATH -x OPEN_MPIBIN_PATH \
                      -x PBS_NODEFILE \
                      -x UCX_RUNTIME_VERSION \
                      -x LD_LIBRARY_PATH \
                      -x PATH  \
                      -x EXE \
                      -x NPROC \
                      -x DPLACE_PIN \
                      -x UCX_TLS \
                      -x OMPI_MCA_rmaps_rank_file_physical \
                      -x UCX_RC_VERBS_RX_MAX_BUFS=1280000000 \
                      -x UCX_WARN_UNUSED_ENV_VARS=n \
                      --use-hwthread-cpus  --oversubscribe \
                      -machinefile ${PBS_NODEFILE}  --mca pml ucx"
                     #-machinefile ${PBS_NODEFILE}  --mca pml ucx -mca osc ucx --mca scoll ucx --mca atomic ucx
                     #-machinefile ${PBS_NODEFILE}  -mca coll_hcoll_enable 1 -x HCOLL_ML_BUFFER_SIZE=65536 -x HCOLL_ENABLE_NBC=1 -x HCOLL_ENABLE_SHARP=1 -x HCOLL_BCOL_P2P_ALLREDUCE_SHARP_MAX=4096 --mca pml ucx "
                     #-machinefile ${PBS_NODEFILE}  -mca coll_hcoll_enable 1 -x HCOLL_ENABLE_NBC=1 -x HCOLL_ENABLE_SHARP=1 --mca pml ucx "
                     #-machinefile ${PBS_NODEFILE}  -mca coll_hcoll_enable 0 --mca pml ucx "
                     #-machinefile ${PBS_NODEFILE}  -mca coll_hcoll_enable 0 --mca btl_openib_allow_ib true"
           if [ -z "$OPENSS_PROF" ]; then
              echo "mpirun -np $NPROC $mpiopts $DPLACE_PIN $EXE $FOAM_OPTIONS -parallel -case ${SCRATCH_DIR}/${NPROC}_d" >> ${LOGFILE} 2>&1
              mpirun -np $NPROC $mpiopts $DPLACE_PIN $EXE $FOAM_OPTIONS -parallel -case ${SCRATCH_DIR}/${NPROC}_d < /dev/null  >> ${LOGFILE} 2>&1
           #
           else
             echo "*** Running OPENMPI with Open Speedshop profiling using OPENSS_TYPE=$OPENSS_TYPE" >> ${LOGFILE}
             echo "*** DBG WHICH_PROF=$WHICH_PROF " >> ${LOGFILE}
             echo "*** DBG $OPENSS_TYPE " `which mpirun` -np $NPROC $mpiopts $DPLACE_PIN `which $EXE` $FOAM_OPTIONS -parallel -case ${SCRATCH_DIR}/${NPROC}_d" >> ${LOGFILE}
             echo "***" >> ${LOGFILE}
	     export OPENSS_PCSAMP_RATE=100
	     export HWLOC_HIDE_ERRORS=1
             $OPENSS_TYPE "`which mpirun` -np $NPROC $mpiopts $DPLACE_PIN `which $EXE` $FOAM_OPTIONS -parallel -case ${SCRATCH_DIR}/${NPROC}_d < /dev/null"" >> ${LOGFILE} 2>&1
           fi
    #   fi # End of Rank File method section
    fi # End of None DEVSHM 

elif [ "$WM_MPLIB" = "HPCX_OPENMPI" ]; then
      echo "Loading & Running with HPCX_OPENMPI"
      echo " "
      echo "Using installation built-in HPCX OPENMPI" >> $LOGFILE
      if [ ! -d $HPCX_HOME ]; then
		    echo "*** HPCX_HOME $HPCX_HOME not found ..." >> $LOGFILE
		    echo "Exit"
		    exit
      fi
      echo "$HPCX_HOME/hpcx-init.sh " >> $LOGFILE
      source $HPCX_HOME/hpcx-init.sh
      hpcx_load
      echo "*** HPCX OPENNMPI mpirun= `which mpirun`" >> $LOGFILE
     # assumed to have been sourced earlier from setup.sh
      export OPEN_MPI_PATH=$MPI_ROOT
     #export OPAL_PREFIX=$MPI_ROOT
      export OPEN_MPILIB_PATH=$OPEN_MPI_PATH/lib
      export OPEN_MPIBIN_PATH=$OPEN_MPI_PATH/bin
      export MPI_BUFFER_SIZE=50000000000
      if [ -n "$BY_NODE" ]; then
         echo "*** Arranging machinefile order by Node ***" >> $LOGFILE
         entries=`wc -l $PBS_NODEFILE| awk '{print $1}'`
         echo DBG: entries=$entries > tmp.out
         cat $PBS_NODEFILE| uniq > BLK_FILE
         blk_len=$NUM_NODES
         num_blks=`expr $entries / $blk_len`
         echo DBG blk_len=$blk_len num_blks=$num_blks >> tmp.out
         rm -f BY_NODE_NODEFILE
         touch BY_NODE_NODEFILE
         for j in `seq 1  $num_blks`
         do
             cat BLK_FILE >> BY_NODE_NODEFILE
         done
         export PBS_NODEFILE=BY_NODE_NODEFILE
      fi
#
      
      echo "BIND_TO=$BIND_TO    DPLACE_PIN=$DPLACE_PIN" >> ${LOGFILE}
      # cat ${PBS_NODEFILE} >> ${LOGFILE}
      echo "*** Using UCX_RUNTIME_VERSION=$UCX_RUNTIME_VERSION" >> ${LOGFILE}
      export LD_LIBRARY_PATH="$WM_THIRD_PARTY_DIR/UCX/$UCX_RUNTIME_VERSION/lib:$WM_THIRD_PARTY_DIR/UCX/$UCX_RUNTIME_VERSION/lib/ucx:$LD_LIBRARY_PATH"
      echo "*** LD_LIBRARY_PATH=$LD_LIBRARY_PATH " >> ${LOGFILE}
      export MACHINE_FILE=${PBS_NODEFILE}
      ntasks=`cat $MACHINE_FILE | wc -l`
      echo "*** Number of lines in MACHINE_FILE = $ntasks " >> ${LOGFILE}
      # see sort -u /sys/devices/system/cpu/cpu*/cache/index3/shared_cpu_list | wc -l 
      # if 16 Milan
      # if 32 7702
      if [ -n "$COPIES" ]; then
         for i in $COPIES_LIST
         do
	   export PROCESSOR_CCX="`sort -u /sys/devices/system/cpu/cpu*/cache/index3/shared_cpu_list | wc -l`"
	   if [ -z "$CCX_TEST" ]; then
		  export CCX_TEST=1
           fi
           echo "*** Using $PROCESSOR_CCX CCXs" >> $LOGFILE
	   if [ "$PROCESSOR_CCX" = "32" -a "$CCX_TEST" = "1" ]; then
        	if [ "$i" = "1" ]; then
          		export CORES_LIST="0 1 2 3 16 17 18 19 32 33 34 35 48 49 50 51 64 65 66 67 80 81 82 83 96 97 98 99 112 113 114 115"
                elif [ "$i" = "2" ]; then
        		export CORES_LIST="4 5 6 7 20 21 22 23 36 37 38 39 52 53 54 55 68 69 70 71 84 85 86 87 100 101 102 103 116 117 118 119"
                elif [ "$i" = "3" ]; then
      	        	export CORES_LIST="8 9 10 11 24 25 26 27 40 41 42 43 56 57 58 59 72 73 74 75 88 89 90 91 104 105 106 107 120 121 122 123"
                elif [ "$i" = "4" ]; then
      	        	export CORES_LIST="12 13 14 15 28 29 30 31 44 45 46 47 60 61 62 63 76 77 78 79 92 93 94 95 108 109 110 111 124 125 126 127"
                fi
           elif [ "$PROCESSOR_CCX" = "32" -a "$CCX_TEST" = "2" ]; then
                if [ "$i" = "1" ]; then
                        export CORES_LIST="0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79"
                elif [ "$i" = "2" ]; then
                        export CORES_LIST="16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95"
                elif [ "$i" = "3" ]; then
                        export CORES_LIST="32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111"
                elif [ "$i" = "4" ]; then
                        export CORES_LIST="48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127"
                fi
           elif [ "$PROCESSOR_CCX" = "16" ]; then
                if [ "$i" = "1" ]; then
                        export CORES_LIST="0   1   2   3   4   5   6   7 16  17  18  19  20  21  22  23 32  33  34  35  36  37  38  39 48  49  50  51  52  53  54  55 64  65  66  67  68  69  70  71 80  81  82  83  84  85  86  87 96  97  98  99 100 101 102 103 112 113 114 115 116 117 118 119 "
                elif [ "$i" = "2" ]; then
                        export CORES_LIST="8   9  10  11  12  13  14  15 24  25  26  27  28  29  30  31 40  41  42  43  44  45  46  47 56  57  58  59  60  61  62  63 72  73  74  75  76  77  78  79 88  89  90  91  92  93  94  95 104 105 106 107 108 109 110 111 120 121 122 123 124 125 126 127"
                fi
           else
                echo "*** Unkown or Unsupported PROCESSOR_CCX $PROCESSOR_CCX value, Exiting ..." >> $LOGFILE
		exit
           fi

           cd ${SCRATCH_DIR}/${NPROC}_d_${i} 
           rm -f ./rk.file
           rank=0
           echo "*** COPY $i : CORES_LIST=`echo ${CORES_LIST}`"  > $LOGFILE_COPY_${i}_of_${COPIES}
           for node_name in `uniq $MACHINE_FILE`
           do
            for j in `echo ${CORES_LIST}`
            do
                # if [ $rank -lt $ntasks ]; then
                   echo "rank ${rank}=${node_name} slot=${j}" >> ./rk.file
                   ((rank=rank+1))
                # fi
            done
           done
   #
          export OMPI_MCA_rmaps_rank_file_physical=1
          export DPLACE_PIN="--bind-to hwthread --rankfile ./rk.file"
          export RANK_FILE=./rk.file
          echo "*** Number of lines in the rank file ="`wc -l $RANK_FILE `"" >> ${LOGFILE}_COPY_${i}_of_${COPIES}
          echo "*** BIND_TO=$BIND_TO    DPLACE_PIN=$DPLACE_PIN" >> ${LOGFILE}_COPY_${i}_of_${COPIES}
          export UCX_TLS="rc_x,mm"
          export HPCX_SHARP_DIR=$HPCX_HOME/sharp
          export OMPI_HOME=$HPCX_HOME/ompi
          mpiopts=" \
                    -x FOAM_SETTINGS -x WM_PROJECT_DIR -x MPI_BUFFER_SIZE \
                    -x OPEN_MPI_PATH -x MPI_ROOT -x OPAL_PREFIX -x OPEN_MPILIB_PATH -x OPEN_MPIBIN_PATH \
                    -x PBS_NODEFILE \
   	  	    -x UCX_RUNTIME_VERSION \
                    -x LD_LIBRARY_PATH \
                    -x PATH  \
                    -x EXE \
                    -x NPROC \
                    -x DPLACE_PIN \
                    -x UCX_TLS="rc_x,mm"  \
                    -x OMPI_MCA_rmaps_rank_file_physical \
                    -x UCX_RC_VERBS_RX_MAX_BUFS=12800000 \
                    -x UCX_WARN_UNUSED_ENV_VARS=n \
   	   	    --use-hwthread-cpus  --oversubscribe \
                    --mca pml ucx"
                  # -machinefile ${PBS_NODEFILE} --mca pml ucx"
          echo "*** Submitting COPY $i:  mpirun -np $NPROC $mpiopts $DPLACE_PIN $EXE $FOAM_OPTIONS -parallel -case ${SCRATCH_DIR}/${NPROC}_d_${i}" >> ${LOGFILE}_COPY_${i}_of_${COPIES}
   	 
            mpirun -np $NPROC $mpiopts $DPLACE_PIN $EXE $FOAM_OPTIONS -parallel -case ${SCRATCH_DIR}/${NPROC}_d_${i} < /dev/null  >> ${LOGFILE}_COPY_${i}_of_${COPIES} &
          done
          wait
      else
      # Single Copy Section
      rm -rf `pwd`/rk.file
      if [ -n "$RANKFILE_USE_OLD_CODE" -o -n "$USER_PPN" ]; then
      #
      # Old code
      #
         if [ -z "$BY_NODE" ]; then
          rank=0
          echo "*** CORES_LIST=$CORES_LIST" >> $LOGFILE >> $LOGFILE
          for node_name in `uniq $MACHINE_FILE`
          do
           for i in `echo $CORES_LIST`
           do
                  if [ $rank -lt $ntasks ]; then
                   echo "rank ${rank}=${node_name} slot=${i}" >> `pwd`/rk.file
                   ((rank=rank+1))
                  fi
           done
          done
         else
          echo "*** Generating rank file on a BY NODE basis" >> $LOGFILE
          echo "*** CORES_LIST=$CORES_LIST" >> $LOGFILE
          rank=0
          for i in `echo $CORES_LIST`
          do
              for j in `uniq BLK_FILE`
              do
                 echo "rank ${rank}=${j} slot=${i}" >> `pwd`/rk.file
                 ((rank=rank+1))
              done
          done
         fi
      else
     # New code
        hosts_list=""
        host_token="0"
        slot=$(expr 0)
        rank=$(expr 0)
        for i in `cat $MACHINE_FILE`
        do
           hosts_list="$hosts_list $i"
        done
        for i in $hosts_list
        do

           if [ "$host_token" = "0" ]; then
             # this is the first name in the list of hosts
             host_token="$i"
             rank=$(expr 0)
             slot=$(expr 0)

           fi
           if [ "$host_token" = "$i" ]; then
             # this is a repeated host list in the sequence
             #

             echo "rank ${rank}=${host_token} slot=${slot}" >> `pwd`/rk.file

           else
             # a new hostname has now appeared in the list
             # thus reset slot to zero and update the hosts_list with the new host_token
             #
             slot=$(expr 0)

             host_token="$i"
             echo "rank ${rank}=${host_token} slot=${slot}" >> `pwd`/rk.file

           fi
           if [ "$ALT" = 1 ]; then
                 ((slot=slot+1))
           elif [ "$ALT" = "2" ]; then
                 ((slot=slot+2))
           elif [ "$ALT" = "4" ]; then
                 ((slot=slot+4))
           elif [ "$ALT" = "8" ]; then
                 ((slot=slot+8))
           elif [ "$ALT" = "16" ]; then
                 ((slot=slot+16))
           elif [ "$ALT" = "32" ]; then
                 ((slot=slot+32))
           else
                 ((slot=slot+1))
           fi
           ((rank=rank+1))
        done
#     End of new code
      fi
      export OMPI_MCA_rmaps_rank_file_physical=1
      echo "mpiopts=$mpiopts" >> ${LOGFILE} 2>&1
      export DPLACE_PIN="--bind-to hwthread --rankfile `pwd`/rk.file"
      export RANK_FILE=`pwd`/rk.file
      echo "*** Number of lines in the rank file ="`wc -l $RANK_FILE `"" >> ${LOGFILE}
      echo "BIND_TO=$BIND_TO    DPLACE_PIN=$DPLACE_PIN" >> ${LOGFILE}
      export HPCX_SHARP_DIR=$HPCX_HOME/sharp
      export OMPI_HOME=$HPCX_HOME/ompi
      if [ "$SLURM_CLUSTER_NAME" = "hotlum" ]; then
         # This is for slingshot like systems
         export UCX_TLS="rc_x,mm"
         export OMPI_MCA_rmaps_rank_file_physical=1
         echo "mpiopts=$mpiopts" >> ${LOGFILE} 2>&1
         export DPLACE_PIN="--bind-to hwthread --rankfile `pwd`/rk.file"
         export RANK_FILE=`pwd`/rk.file
         echo "*** Number of lines in the rank file ="`wc -l $RANK_FILE `"" >> ${LOGFILE}
         echo "BIND_TO=$BIND_TO    DPLACE_PIN=$DPLACE_PIN" >> ${LOGFILE}
         export HPCX_SHARP_DIR=$HPCX_HOME/sharp
         export OMPI_HOME=$HPCX_HOME/ompi
         mpiopts="$mpiopts \
                 -x FOAM_SETTINGS -x WM_PROJECT_DIR -x MPI_BUFFER_SIZE \
                 -x OPEN_MPI_PATH -x MPI_ROOT -x OPAL_PREFIX -x OPEN_MPILIB_PATH -x OPEN_MPIBIN_PATH \
                 -x PBS_NODEFILE \
                 -x UCX_RUNTIME_VERSION \
                 -x LD_LIBRARY_PATH \
                 -x PATH  \
                 -x EXE \
                 -x NPROC \
                 -x DPLACE_PIN \
                 -x UCX_TLS \
                 -x OMPI_MCA_rmaps_rank_file_physical \
                 -x UCX_WARN_UNUSED_ENV_VARS=n \
                 --use-hwthread-cpus  --oversubscribe \
                 -machinefile ${PBS_NODEFILE}  --mca coll_hcoll_enable 0 --mca pml ucx"
      else
         # This is for ib mlx like systems
         mpiopts="$mpiopts \
                 -x FOAM_SETTINGS -x WM_PROJECT_DIR -x MPI_BUFFER_SIZE \
                 -x OPEN_MPI_PATH -x MPI_ROOT -x OPAL_PREFIX -x OPEN_MPILIB_PATH -x OPEN_MPIBIN_PATH \
                 -x PBS_NODEFILE \
                 -x UCX_RUNTIME_VERSION \
                 -x LD_LIBRARY_PATH \
                 -x PATH  \
                 -x EXE \
                 -x NPROC \
                 -x DPLACE_PIN \
                 -x UCX_TLS=rc_x,mm \
                 -x OMPI_MCA_rmaps_rank_file_physical \
                 -x UCX_RC_VERBS_RX_MAX_BUFS=1280000000 \
                 -x UCX_WARN_UNUSED_ENV_VARS=n \
                 -x UCX_RNDV_THRESH=65536 -x UCX_ZCOPY_THRESH=131072 \
		 -x OMPI_MCA_opal_common_ucx_opal_mem_hooks=1 \
                 -x UCX_MAX_EAGER_LANES=2 \
                 -x UCX_MAX_RNDV_LANES=2 \
                 --use-hwthread-cpus  --oversubscribe \
                 -machinefile ${PBS_NODEFILE}  --mca pml ucx -mca osc ucx -mca coll_hcoll_enable 0 \
		  --mca btl_openib_allow_ib 1 --mca btl_openib_max_send_size 306144 "
	       # -x UCX_NET_DEVICES=mlx5_0:1,mlx5_1:1 \
		 #-x UCX_RC_MLX5_TM_ENABLE=y \
                 #-machinefile ${PBS_NODEFILE}  -mca coll_hcoll_enable 1 -x HCOLL_ML_BUFFER_SIZE=65536 -x HCOLL_ENABLE_NBC=1 -x HCOLL_ENABLE_SHARP=1 -x HCOLL_BCOL_P2P_ALLREDUCE_SHARP_MAX=4096"
      fi
      if [ -z "$OPENSS_PROF" ]; then
         echo "mpirun -np $NPROC $mpiopts $DPLACE_PIN $EXE $FOAM_OPTIONS -parallel -case ${SCRATCH_DIR}/${NPROC}_d" >> ${LOGFILE} 2>&1
         mpirun -np $NPROC $mpiopts $DPLACE_PIN $EXE $FOAM_OPTIONS -parallel -case ${SCRATCH_DIR}/${NPROC}_d < /dev/null  >> ${LOGFILE} 2>&1
      #
      else
        echo "*** Running HPCX OPENMPI with Open Speedshop profiling using OPENSS_TYPE=$OPENSS_TYPE" >> ${LOGFILE}
        echo "***" >> ${LOGFILE}
        echo "*** DBG ${WHICH_PROF} `which mpirun` -np $NPROC $mpiopts $DPLACE_PIN `which $EXE` $FOAM_OPTIONS -parallel -case ${SCRATCH_DIR}/${NPROC}_d" >> ${LOGFILE}
        echo "***" >> ${LOGFILE}
       `which ${WHICH_PROF}` "`which mpirun` -np $NPROC $mpiopts $DPLACE_PIN `which $EXE` $FOAM_OPTIONS -parallel -case ${SCRATCH_DIR}/${NPROC}_d < /dev/null"  >> ${LOGFILE} 2>&1
      fi
      fi

elif [ "$WM_MPLIB" = "CRAY-MPICH" ]; then
      echo "Running OpenFOAM $EXE using CRAY-MPICH mpi" >> $LOGFILE
      if [ -n "$BY_NODE" ]; then
         echo "*** Arranging machinefile order by Node ***" >> $LOGFILE
         entries=`wc -l $PBS_NODEFILE| awk '{print $1}'`
         echo DBG: entries=$entries > tmp.out
         cat $PBS_NODEFILE| uniq > BLK_FILE
         blk_len=$NUM_NODES
         num_blks=`expr $entries / $blk_len`
         echo DBG blk_len=$blk_len num_blks=$num_blks >> tmp.out
         rm -f BY_NODE_NODEFILE
         touch BY_NODE_NODEFILE
         for j in `seq 1  $num_blks`
         do
             cat BLK_FILE >> BY_NODE_NODEFILE
         done
         export PBS_NODEFILE=BY_NODE_NODEFILE
      fi
#
#export FI_CXI_RX_MATCH_MODE=software
#export FI_CXI_REQ_BUF_MIN_POSTED=10
#export FI_CXI_REQ_BUF_SIZE=25165824
#export FI_CXI_DEFAULT_CQ_SIZE=131072
#export FI_CXI_RDZV_THRESHOLD=131072
#export MPICH_OFI_NIC_POLICY="BLOCK"
#export MPICH_OFI_NIC_MAPPING=1
#export MPICH_ALLREDUCE_NO_SMP=0
#
# very bad setting export MPICH_OFI_USE_PROVIDER="ofi_rxm"
#export FI_CXI_REQ_BUF_MIN_POSTED=8


      
      echo "*** CRAY-MPICH CORES=$CORES" >> ${LOGFILE} 2>&1
      if [ -n "$CRAY_MPICH_USE_SRUN" ]; then
#        echo " srun -t 20:00:00 -n $NPROC -m cyclic --cpu-bind=map_cpu:`seq -s, 0 127` $EXE $FOAM_OPTIONS -parallel -case ${SCRATCH_DIR}/${NPROC}" >> ${LOGFILE} 2>&1
#        srun -n $NPROC -m block:block --cpu-bind=verbose $EXE $FOAM_OPTIONS -parallel -case ${SCRATCH_DIR}/${NPROC}_d < /dev/null  >> ${LOGFILE} 2>&1
#        srun -n $NPROC -m block:cyclic --cpu-bind=verbose $EXE $FOAM_OPTIONS -parallel -case ${SCRATCH_DIR}/${NPROC}_d < /dev/null  >> ${LOGFILE} 2>&1
#        srun -n $NPROC -m cyclic:block --cpu-bind=verbose $EXE $FOAM_OPTIONS -parallel -case ${SCRATCH_DIR}/${NPROC}_d < /dev/null  >> ${LOGFILE} 2>&1
#        srun -n $NPROC -m cyclic:cyclic --cpu-bind=verbose $EXE $FOAM_OPTIONS -parallel -case ${SCRATCH_DIR}/${NPROC}_d < /dev/null  >> ${LOGFILE} 2>&1
        srun -n $NPROC --cpu-bind=verbose $EXE $FOAM_OPTIONS -parallel -case ${SCRATCH_DIR}/${NPROC}_d < /dev/null  >> ${LOGFILE} 2>&1

      elif [ -n "$CRAY_MPICH_USE_MPIEXEC" ]; then
        # CRAY-MPICH uses PBS using mpiexec
        module load cray-pals
        echo " *** Running Cray mpich command line:" >> ${LOGFILE}
        echo " mpiexec -n $NPROC $DPLACE_BIND_TO $EXE $FOAM_OPTIONS -parallel -case ${SCRATCH_DIR}/${NPROC}_d " >> ${LOGFILE}
	echo " *** " >> ${LOGFILE}
        export RFE_811452_DISABLE=1
        mpiexec -n $NPROC $DPLACE_BIND_TO `which $EXE` $FOAM_OPTIONS -parallel -case ${SCRATCH_DIR}/${NPROC}_d < /dev/null  >> ${LOGFILE} 2>&1
      else
         # CRAY-MPICH uses PBS using aprun
         module load cray-pals
         aprun -n $NPROC $DPLACE_BIND_TO `which $EXE` $FOAM_OPTIONS -parallel -case ${SCRATCH_DIR}/${NPROC}_d < /dev/null  >> ${LOGFILE} 2>&1
      fi
     #
elif [ "$WM_MPLIB" = "INTELMPI" ]; then
#
     cat ${PBS_NODEFILE} > hosts
   # export _LMFILES_=/sw/com/modulefiles/intel-mpi/5.1.3.181
   # . /usr/share/Modules/init/bash
   # module use /sw/com/modulefiles
   # module load intel-mpi-20/19.1.3.304
    #export I_MPI_DEVICE=rdssm:OpenIB-cma
    #export I_MPI_DEVICE=rdssm:OpenIB-cma
  # export FI_PROVIDER=tcp
  # export FI_SOCKETS_IFACE=eth0
    export I_MPI_PLATFORM auto
    ldd `which simpleFoam`

   #export I_MPI_DEVICE=shm:ofi
    #
    export FABRIC_OPTIONS=""
    if [ "$FABRIC" = "OPA" ]; then
         # export FABRIC_OPTIONS="-PSM2 -genv PSM2_MQ_EAGER_SDMA_SZ 32768 -genv PSM2_MQ_RNDV_HFI_WINDOW 524288"
         # export FABRIC_OPTIONS="-PSM2 -genv PSM2_MQ_EAGER_SDMA_SZ 32768 "
         # export FABRIC_OPTIONS="-PSM2 -genv PSM2_MQ_RNDV_HFI_WINDOW 524288"
           export FABRIC_OPTIONS="-PSM"
    else
           
          # DO NOT DELETE THE TWO LINES BELOW !!!
           export FABRIC_OPTIONS="--rsh=ssh -genv I_MPI_FALLBACK_DEVICE disable -genv I_MPI_DEBUG 0 -genv I_MPI_ADJUST_REDUCE 2 -genv I_MPI_ADJUST_ALLREDUCE 2 -genv I_MPI_ADJUST_BCAST 1 -genv I_MPI_ADJUST_BARRIER 2 -genv I_MPI_ADJUST_ALLGATHER 2 -genv I_MPI_ADJUST_GATHER 2 -genv I_MPI_ADJUST_ALLTOALL 1 -genv I_MPI_ADJUST_SCATTER 2 -genv I_MPI_ADJUST_SCATTERV 2 -genv I_MPI_ADJUST_ALLGATHERV 2 -genv I_MPI_ADJUST_GATHERV 2 -genv I_MPI_PLATFORM auto -genv KMP_AFFINITY disabled"
          #export FABRIC_OPTIONS="--rsh=ssh -genv FI_PROVIDER=tcp -genv I_MPI_ADJUST_ALLREDUCE 2"
   #export PATH=/sw/sdev/intel/parallel_studio_xe_2020_update4/compilers_and_libraries_2020.4.304/linux/mpi/intel64/libfabric/bin:/sw/sdev/intel/parallel_studio_xe_2020_update4/compilers_and_libraries_2020.4.304/linux/mpi/intel64/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/c3/bin:/opt/pbs/bin:/sbin:/bin:/usr3/ayad/bin:/usr/local/bin:/usr/bin:/usr/bsd:/bin:/etc:.:$PATH
   #export LD_LIBRARY_PATH=/sw/sdev/intel/parallel_studio_xe_2020_update4/compilers_and_libraries_2020.4.304/linux/mpi/intel64/libfabric/lib:/sw/sdev/intel/parallel_studio_xe_2020_update4/compilers_and_libraries_2020.4.304/linux/mpi/intel64/lib/release:/sw/sdev/intel/parallel_studio_xe_2020_update4/compilers_and_libraries_2020.4.304/linux/mpi/intel64/lib:$LD_LIBRARY_PATH
       #export FABRIC_OPTIONS="-genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH -genv FI_PROVIDER=mlx -genv I_MPI_ADJUST_ALLREDUCE 1"
      # . /usr/share/Modules/init/bash
      # module use /sw/com/modulefiles
      # module load intel-mpi-20/19.1.3.304
      # module load intel-oneapi/2021.1.0/mpi/latest
        export _LMFILES_=/sw/com/modulefiles/intel-oneapi-2021/mpi/latest
        which mpirun
        export LD_LIBRARY_PATH=$WM_THIRD_PARTY_DIR/intelmpi/19.1.3.304/intel64/libfabric/lib:$LD_LIBRARY_PATH
        export FABRIC_OPTIONS="$FABRIC_OPTIONS -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH -genv FI_PROVIDER=mlx -genv I_MPI_ADJUST_ALLREDUCE 1"
        echo "*** Using FABRIC_OPTIONS=$FABRIC_OPTIONS" >> ${LOGFILE}
    fi
    echo "mpirun -np $NPROC $FABRIC_OPTIONS --machinefile ${PBS_NODEFILE} $AFIN `which $EXE` -parallel " >> ${LOGFILE}
    echo ""
    env | grep CPU_AFFINITY_ALLOW_HT >> ${LOGFILE}
    mpirun -np $NPROC $FABRIC_OPTIONS -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH --machinefile ${PBS_NODEFILE} \
            $AFIN \
           `which $EXE` \
            -parallel < /dev/null >> ${LOGFILE} 2>&1
fi
#
echo "Job started at `date`" >> Job_Start_End_Dates_${NPROC}cores
slurm_cleanup
